{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from My_Transformers.ipynb\n",
      "importing Jupyter notebook from Word2Vec_Transformer.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-03 15:35:44,773 INFO adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-10-03 15:35:44,774 INFO built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "import import_ipynb\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import matplotlib.pyplot as plt\n",
    "import random  \n",
    "from random import sample \n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "import string  \n",
    "from bert_serving.client import BertClient\n",
    "\n",
    "from torch import nn\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import WarmRestartLR, LRScheduler, EarlyStopping\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "from sklearn.compose import make_column_selector\n",
    "# from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from mlxtend.classifier import StackingClassifier, StackingCVClassifier\n",
    "from sklearn.decomposition import PCA, NMF, KernelPCA, LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import My_Transformers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Global var\n",
    "TRAIN_ROW_COUNT = 8234\n",
    "from My_Transformers import *\n",
    "from Word2Vec_Transformer import Text_cleaner_transformer, STOPWORDS\n",
    "import warnings\n",
    "warnings.filterwarnings (\"ignore\")\n",
    "\n",
    "# Global vars\n",
    "VERSION = 9\n",
    "trainDF, testDF, evalDF = None, None, None\n",
    "sclf_1_1, sclf_1_2, sclf_1_3, sclf_2_1 = None, None, None, None\n",
    "lsvc_1, lsvc_2, lsvc_3, lsvc_4, catb_1, catb_2, catb_3, catb_4, fcnn_1, fcnn_2 = None, None, None, None, None, None, None, None, None, None\n",
    "X_train, X_test, X_eval, tfidf_slice, bert_meta_slice, bert_meta_ica_slice, tfidf_pca_nmf_slice = None, None, None, None, None, None, None\n",
    "TRAIN_SAMPLE_WEIGHT = None\n",
    "CACHEDIR = mkdtemp()\n",
    "BEST_CLF = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickleSave (obj, file):\n",
    "\n",
    "    if VERSION != '':\n",
    "\n",
    "        file, ext = os.path.splitext (file)\n",
    "        file += \"_v\" + str (VERSION) + ext\n",
    "    dirs = os.path.dirname (file) \n",
    "    if dirs:\n",
    "        os.makedirs (dirs, exist_ok=True)\n",
    "    with open (file, 'wb') as f:\n",
    "        pickle.dump (obj, f)\n",
    "    return\n",
    "\n",
    "def unpickle (file):\n",
    "\n",
    "    if VERSION != '':\n",
    "\n",
    "        file, ext = os.path.splitext(file)\n",
    "        file += \"_v\" + str(VERSION) + ext\n",
    "    return pickle.load (open (file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discrete2OneHot_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.LB_1 = None\n",
    "        self.LB_2 = None\n",
    "        return\n",
    "\n",
    "    def fit (self, X, y=None, **fit_params):\n",
    "        \n",
    "        print ('Discrete2OneHot_FeatureTransformer: fit(): X.shape =', X.shape)\n",
    "        self.LB_1 = LabelBinarizer ()\n",
    "        X['keyword'] = X.keyword.astype (str)\n",
    "        self.LB_1.fit (list (X['keyword']))\n",
    "        # self.LB_2 = LabelBinarizer ()\n",
    "        # self.LB_2.fit (list (pd.isnull (X['location'])))\n",
    "        return self\n",
    "\n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        X['keyword'] = X.keyword.astype (str)\n",
    "        A = self.LB_1.transform (list (X['keyword']))\n",
    "        # B = self.LB_2.transform (list (pd.isnull (X['location']))).reshape (-1, 1)\n",
    "        # X = np.hstack ((A, B))\n",
    "        X = A\n",
    "        print ('Discrete2OneHot_FeatureTransformer: transform(): X.shape =', X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FV Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vars\n",
    "\n",
    "tfidf_ft = Pipeline ([ \n",
    "    \n",
    "    ('tfidf_text_meta_keyword', ColumnTransformer ([ \n",
    "    \n",
    "        ('tfidf_text', Pipeline ([\n",
    "\n",
    "            ('tfidf_vectorizer',        TfidfVectorizer (analyzer='char', ngram_range=(3, 8), max_df=0.90, min_df=5, token_pattern='(\\S+)')),\n",
    "            ('sparse_svm_feat_select1', SparseSVM_feature_selector ()),\n",
    "            ('interactions',            PolynomialFeatures (2, interaction_only=True, include_bias=False)),\n",
    "            ('standardization1',        StandardScaler (with_mean=False)),\n",
    "            ('sparse_svm_feat_select2', SparseSVM_feature_selector ()),\n",
    "        ], memory=CACHEDIR), 'text_cleaned'),\n",
    "        # ('tfidf_location',  TfidfVectorizer (analyzer='char', ngram_range=(3, 7), max_df=0.90, min_df=5, token_pattern='(\\S+)'), 'location'),\n",
    "        ('meta_features', 'passthrough', ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length', 'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']),\n",
    "        ('keyword',        Discrete2OneHot_FeatureTransformer (), ['keyword']),\n",
    "    ])), \n",
    "    ('cosineSim',         Cosine_sim_ratios_Features ()),\n",
    "    ('standardization2',  StandardScaler (with_mean=False)),\n",
    "    ('k_best',            SelectKBest_feature_selector (n_components=[0.8, 0.9, 0.95], score_func=[mutual_info_classif]))\n",
    "], memory=CACHEDIR)\n",
    "\n",
    "tfidf_pca_nmf_ft = Pipeline ([ \n",
    "\n",
    "    ('tfidf_ft', tfidf_ft),\n",
    "    ('pca_nmf',  PCA_NMF_TrainTest_FeatureTransformer (isSparseOut=True)),\n",
    "    ('k_best',   SelectKBest_feature_selector (n_components=[0.8, 0.9, 0.95], score_func=[mutual_info_classif]))\n",
    "], memory=CACHEDIR)\n",
    "\n",
    "bert_ft = ColumnTransformer ([\n",
    "    \n",
    "    ('bert', 'passthrough', make_column_selector (r'bert')),\n",
    "])\n",
    "\n",
    "bert_meta_ft = ColumnTransformer ([\n",
    "    \n",
    "    ('bert',          'passthrough', make_column_selector (r'bert')),\n",
    "    # ('keyword',        Discrete2OneHot_FeatureTransformer (), ['keyword']),\n",
    "    ('meta_features', 'passthrough', ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length', 'char_count', 'punctuation_count', 'hashtag_count', 'mention_count'])\n",
    "])\n",
    "\n",
    "bert_meta_keyword_ica_ft = Pipeline ([\n",
    "    \n",
    "    ('bert_keyword_meta', ColumnTransformer ([ \n",
    "        \n",
    "        ('bert',          'passthrough', make_column_selector (r'bert')),\n",
    "        ('keyword',        Discrete2OneHot_FeatureTransformer (), ['keyword']),\n",
    "        ('meta_features', 'passthrough', ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length', 'char_count', 'punctuation_count', 'hashtag_count', 'mention_count'])\n",
    "    ])),\n",
    "    ('cosineSim',  Cosine_sim_ratios_Features ()),\n",
    "    ('kpca_ica',  FeatureUnion([\n",
    "        \n",
    "        ('kpca',   KPCA_TrainTest_FeatureTransformer (kernel=['cosine'], isSparseOut=True)),\n",
    "        ('ica',    ICA_TrainTest_FeatureTransformer (isSparseOut=True))\n",
    "    ])),\n",
    "    ('k_best',     SelectKBest_feature_selector (n_components=[0.8, 0.9, 0.95], score_func=[mutual_info_classif]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform_fv (df):\n",
    "    \n",
    "    global tfidf_ft, bert_meta_ft, bert_meta_keyword_ica_ft, tfidf_pca_nmf_ft\n",
    "    global tfidf_slice, bert_meta_slice, bert_meta_ica_slice, tfidf_pca_nmf_slice\n",
    "    \n",
    "    print ('fit_transform_fv () Begin with tfidf_ft.fit_transform ():')\n",
    "    X_tfidf         = tfidf_ft.fit_transform (df, df['target_relabeled'])\n",
    "    print ('bert_meta_ft.fit_transform ():')\n",
    "    X_bert_meta     = bert_meta_ft.fit_transform (df, df['target_relabeled'])\n",
    "    print ('bert_meta_keyword_ica_ft.fit_transform ():')\n",
    "    X_bert_meta_ica = bert_meta_keyword_ica_ft.fit_transform (df, df['target_relabeled'])\n",
    "    print ('tfidf_pca_nmf_ft.fit_transform ():')\n",
    "    X_tfidf_pca_nmf = tfidf_pca_nmf_ft.fit_transform (df, df['target_relabeled'])\n",
    "    \n",
    "    X = scipy.sparse.hstack ([X_tfidf, X_bert_meta, X_bert_meta_ica, X_tfidf_pca_nmf])\n",
    "    X = X.tocsr ()    \n",
    "    tfidf_slice         = slice (0, X_tfidf.shape[1])\n",
    "    bert_meta_slice     = slice (X_tfidf.shape[1],  X_tfidf.shape[1] + X_bert_meta.shape[1])\n",
    "    bert_meta_ica_slice = slice (X_tfidf.shape[1] + X_bert_meta.shape[1], X_tfidf.shape[1] + X_bert_meta.shape[1] + X_bert_meta_ica.shape[1])\n",
    "    tfidf_pca_nmf_slice = slice (X_tfidf.shape[1] + X_bert_meta.shape[1] + X_bert_meta_ica.shape[1], X_tfidf.shape[1] + X_bert_meta.shape[1] + X_bert_meta_ica.shape[1] + X_tfidf_pca_nmf.shape[1])\n",
    "    print ('fit_transform_fv () Done.')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_fv (df):\n",
    "    \n",
    "    global tfidf_ft, bert_meta_ft, bert_meta_keyword_ica_ft, tfidf_pca_nmf_ft\n",
    "    global tfidf_slice, bert_meta_slice, bert_meta_ica_slice, tfidf_pca_nmf_slice\n",
    "    \n",
    "    X_tfidf         = tfidf_ft.transform (df)\n",
    "    X_bert_meta     = bert_meta_ft.transform (df)   \n",
    "    X_bert_meta_ica = bert_meta_keyword_ica_ft.transform (df)\n",
    "    X_tfidf_pca_nmf = tfidf_pca_nmf_ft.transform (df)\n",
    "    \n",
    "    X = scipy.sparse.hstack ([X_tfidf, X_bert_meta, X_bert_meta_ica, X_tfidf_pca_nmf])\n",
    "    X = X.tocsr()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-0 Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fit_L0_clf (X, y):\n",
    "    \n",
    "    global lsvc_1, lsvc_2, lsvc_3, lsvc_4, catb_1, catb_2, catb_3, catb_4, fcnn_1, fcnn_2\n",
    "    global tfidf_slice, bert_meta_slice, bert_meta_ica_slice, tfidf_pca_nmf_slice\n",
    "    \n",
    "    # Create\n",
    "    lsvc_1 = Pipeline ([ \n",
    "        ('col_select', ColumnTransformer ([('col_select', 'passthrough', tfidf_slice) ])),\n",
    "        ('clf',        Best_clf_cv_transformer ({ 'name': 'LSvc', 'n_estimators': 20, 'params': {'penalty': 'l2', 'class_weight': 'balanced'}, 'param_grid': {'C' : [0.001, 0.005, 0.01, 0.05, 0.1, 1, 10]} }))\n",
    "    ])\n",
    "    lsvc_1.name = 'lsvc_1'\n",
    "\n",
    "    lsvc_2 = Pipeline ([ \n",
    "         ('col_select', ColumnTransformer ([('col_select', 'passthrough', bert_meta_slice) ])),\n",
    "         ('clf',        Best_clf_cv_transformer ({ 'name': 'LSvc', 'n_estimators': 20,  'params': {'penalty': 'l2', 'class_weight': 'balanced'}, 'param_grid': {'C' : [0.001, 0.005, 0.01, 0.05, 0.1, 1, 10]} }))\n",
    "    ])\n",
    "    lsvc_2.name = 'lsvc_2'\n",
    "    \n",
    "    lsvc_3 = Pipeline ([ \n",
    "         ('col_select', ColumnTransformer ([('col_select', 'passthrough', bert_meta_ica_slice) ])),\n",
    "         ('clf',        Best_clf_cv_transformer ({ 'name': 'LSvc', 'n_estimators': 20,  'params': {'penalty': 'l2', 'class_weight': 'balanced'}, 'param_grid': {'C' : [0.001, 0.005, 0.01, 0.05, 0.1, 1, 10]} }))\n",
    "    ])\n",
    "    lsvc_3.name = 'lsvc_3'\n",
    "    \n",
    "    lsvc_4 = Pipeline ([ \n",
    "         ('col_select', ColumnTransformer ([('col_select', 'passthrough', tfidf_pca_nmf_slice) ])),\n",
    "         ('clf',        Best_clf_cv_transformer ({ 'name': 'LSvc', 'n_estimators': 20,  'params': {'penalty': 'l2', 'class_weight': 'balanced'}, 'param_grid': {'C' : [0.001, 0.005, 0.01, 0.05, 0.1, 1, 10]} }))\n",
    "    ])\n",
    "    lsvc_4.name = 'lsvc_4'\n",
    "\n",
    "\n",
    "    catb_1 = Pipeline ([ \n",
    "         ('col_select', ColumnTransformer ([('col_select', 'passthrough', tfidf_slice) ])),\n",
    "         ('clf',        Best_clf_cv_transformer ({ 'name': 'Catb',  'isCV': False}))\n",
    "    ])\n",
    "    catb_1.name = 'catb_1'\n",
    "\n",
    "    catb_2 = Pipeline ([ \n",
    "         ('col_select', ColumnTransformer ([('col_select', 'passthrough', bert_meta_slice) ])),\n",
    "         ('clf',        Best_clf_cv_transformer ({ 'name': 'Catb',  'isCV': False}))\n",
    "    ])\n",
    "    catb_2.name = 'catb_2'\n",
    "    \n",
    "    catb_3 = Pipeline ([ \n",
    "         ('col_select', ColumnTransformer ([('col_select', 'passthrough', bert_meta_ica_slice) ])),\n",
    "         ('clf',        Best_clf_cv_transformer ({ 'name': 'Catb',  'isCV': False}))\n",
    "    ])\n",
    "    catb_3.name = 'catb_3'\n",
    "    \n",
    "    catb_4 = Pipeline ([ \n",
    "         ('col_select', ColumnTransformer ([('col_select', 'passthrough', tfidf_pca_nmf_slice) ])),\n",
    "         ('clf',        Best_clf_cv_transformer ({ 'name': 'Catb',  'isCV': False}))\n",
    "    ])\n",
    "    catb_4.name = 'catb_4'\n",
    "    \n",
    "    \n",
    "    fcnn_1 = Pipeline ([ \n",
    "         ('col_select', ColumnTransformer ([('col_select', 'passthrough', bert_meta_slice) ])),\n",
    "         ('clf',        Best_clf_cv_transformer ({ 'name': 'FCNN', 'max_epochs': 100, 'n_estimators': 20}))\n",
    "    ])\n",
    "    fcnn_1.name = 'fcnn_1'\n",
    "    \n",
    "    fcnn_2 = Pipeline ([ \n",
    "         ('col_select', ColumnTransformer ([('col_select', 'passthrough', bert_meta_ica_slice) ])),\n",
    "         ('clf',        Best_clf_cv_transformer ({ 'name': 'FCNN', 'max_epochs': 100, 'n_estimators': 20}))\n",
    "    ])\n",
    "    fcnn_2.name = 'fcnn_2'\n",
    "    \n",
    "    # fit\n",
    "    lsvc_1.fit (X, y) \n",
    "    lsvc_2.fit (X, y) \n",
    "    lsvc_3.fit (X, y) \n",
    "    lsvc_4.fit (X, y) \n",
    "    catb_1.fit (X, y)\n",
    "    catb_2.fit (X, y) \n",
    "    catb_3.fit (X, y)\n",
    "    catb_4.fit (X, y)\n",
    "    fcnn_1.fit (X, y)\n",
    "    fcnn_2.fit (X, y)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fit_stack_clf (X, y):\n",
    "    \n",
    "    global sclf_1_1, sclf_1_2, sclf_1_3, sclf_2_1\n",
    "    global lsvc_1, lsvc_2, lsvc_3, lsvc_4, catb_1, catb_2, catb_3, catb_4, fcnn_1, fcnn_2\n",
    "    \n",
    "    # Level 1 create\n",
    "    sclf_1_1 = StackingClassifier (classifiers=[lsvc_1, lsvc_2, lsvc_3, lsvc_4, catb_1, catb_2, catb_3, catb_4, fcnn_1, fcnn_2], \n",
    "                                   fit_base_estimators=False, use_probas=True, average_probas=False, \n",
    "                                   meta_classifier=Best_clf_cv_transformer ({ 'name': 'LSvc',  'params': {'penalty': 'l2', 'class_weight': 'balanced'}, 'param_grid': {'C' : [0.005, 0.01, 0.05, 0.1, 1, 10]} }) )\n",
    "    sclf_1_1.name = 'sclf_1_1'\n",
    "\n",
    "    sclf_1_2 = StackingClassifier (classifiers=[lsvc_1, lsvc_2, lsvc_3, lsvc_4, catb_1, catb_2, catb_3, catb_4, fcnn_1, fcnn_2], \n",
    "                                   fit_base_estimators=False, use_probas=True, average_probas=False, \n",
    "                                   meta_classifier=Best_clf_cv_transformer ({ 'name': 'RF',    'params': {'class_weight': 'balanced', 'n_estimators': 3, 'max_features': None} }))\n",
    "    sclf_1_2.name = 'sclf_1_2'\n",
    "\n",
    "    sclf_1_3 = StackingClassifier (classifiers=[lsvc_1, lsvc_2, lsvc_3, lsvc_4, catb_1, catb_2, catb_3, catb_4, fcnn_1, fcnn_2], \n",
    "                                   fit_base_estimators=False, use_probas=True, average_probas=False, \n",
    "                                   meta_classifier=Best_clf_cv_transformer ({ 'name': 'Catb',  'isCV': False }))\n",
    "    sclf_1_3.name = 'sclf_1_3'\n",
    "    \n",
    "    # fit\n",
    "    sclf_1_1.fit (X, y)\n",
    "    sclf_1_2.fit (X, y)\n",
    "    sclf_1_3.fit (X, y)\n",
    "\n",
    "    # Level 2 create\n",
    "    sclf_2_1 = StackingClassifier (classifiers=[sclf_1_1, sclf_1_2, sclf_1_3], fit_base_estimators=False, \n",
    "                                   use_probas=True, average_probas=False, \n",
    "                                   meta_classifier=Best_clf_cv_transformer ({ 'name': 'LSvc',  'params': {'penalty': 'l2', 'class_weight': 'balanced'}, 'param_grid': {'C' : [0.005, 0.01, 0.05, 0.1, 1, 10]} }) )\n",
    "    sclf_2_1.name = 'sclf_2_1'\n",
    "    \n",
    "    # fit\n",
    "    sclf_2_1.fit (X, y)\n",
    "    return    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample weights - don't know how to use this ? How to deal with unbalanced classes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_weights (df):\n",
    "    \n",
    "    sample_weights = []\n",
    "    y = df[pd.isnull (df['target_relabeled']) == False]['target_relabeled']\n",
    "    a = 1.0/ np.sum (y)                # class weight for label 1\n",
    "    b = 1.0/ (len (y) - np.sum (y))    # class weight for label 0\n",
    "    w1 = a/(a+b)\n",
    "    w0 = b/(a+b)\n",
    "    for i in y:\n",
    "        if i:\n",
    "            sample_weights.append (w1)\n",
    "        else:\n",
    "            sample_weights.append (w0)\n",
    "    return sample_weights, b, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init ():\n",
    "    \n",
    "    global trainDF, evalDF, TRAIN_ROW_COUNT\n",
    "    \n",
    "    train_file = \"Data/all.tsv\"\n",
    "    rawDF = pd.read_csv (train_file, sep='\\t')  #, dtype={'id': np.int16, 'target': np.int8})\n",
    "    # rawDF = rawDF[0:30]               # Comment this out ********************************\n",
    "    rawDF['text'] = rawDF.text.astype (str)\n",
    "    rawDF['keyword'] = rawDF.keyword.astype (str)\n",
    "    rawDF['location'] = rawDF.location.astype (str)\n",
    "    rawDF['text_cleaned'] = rawDF.text_cleaned.astype (str)\n",
    "    # rawDF['target_relabeled'] = rawDF.target_relabeled.astype (int)\n",
    "    \n",
    "    labeledDF = rawDF[:TRAIN_ROW_COUNT]\n",
    "    trainDF, evalDF = train_test_split (labeledDF, test_size=0.33, random_state=42)\n",
    "    evalDF['target_relabeled'] = evalDF.target_relabeled.astype (int)\n",
    "    trainDF['target_relabeled'] = trainDF.target_relabeled.astype (int)\n",
    "    trainDF_temp = pd.concat ((trainDF, rawDF[TRAIN_ROW_COUNT:]), axis=0)\n",
    "    TRAIN_ROW_COUNT = trainDF.shape[0]\n",
    "    trainDF = trainDF_temp\n",
    "    \n",
    "    print ('rawDF.shape =', rawDF.shape, 'trainDF.shape =', trainDF.shape, 'evalDF.shape =', evalDF.shape)\n",
    "    rawDF, labeledDF, trainDF_temp = None, None, None\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit (clf=None, isFitOnOrigDF=False):\n",
    "    \n",
    "    global trainDF, X_train\n",
    "    \n",
    "    if isFitOnOrigDF:\n",
    "        \n",
    "        train_file = \"Data/all.tsv\"\n",
    "        trainDF = pd.read_csv (train_file, sep='\\t')    #, dtype={'id': np.int16, 'target': np.int8})\n",
    "        trainDF['text'] = trainDF.text.astype (str)\n",
    "        trainDF['keyword'] = trainDF.keyword.astype (str)\n",
    "        trainDF['location'] = trainDF.location.astype (str)\n",
    "        trainDF['text_cleaned'] = trainDF.text_cleaned.astype (str)\n",
    "        # trainDF = trainDF[0:20]               # Comment this out ********************************\n",
    "    X_train = fit_transform_fv (trainDF)    \n",
    "    if clf is None:\n",
    "        \n",
    "        create_fit_L0_clf (X_train, trainDF['target_relabeled'])\n",
    "        create_fit_stack_clf (X_train, trainDF['target_relabeled'])\n",
    "    else:\n",
    "        \n",
    "        clf.fit (X_train, trainDF['target_relabeled'])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict (clf, iterCount=0):\n",
    "    \n",
    "    global testDF, X_test\n",
    "    \n",
    "    print (\"Starting Prediction by\", clf.name)\n",
    "    test_file = \"Data/test.tsv\"\n",
    "    testDF = pd.read_csv (test_file, sep='\\t')   #, dtype={'id': np.int16, 'target': np.int8})\n",
    "    # testDF = testDF[0:10]              # Comment this out ********************************\n",
    "    testDF['text'] = testDF.text.astype (str)\n",
    "    testDF['keyword'] = testDF.keyword.astype (str)\n",
    "    testDF['location'] = testDF.location.astype (str)\n",
    "    testDF['text_cleaned'] = testDF.text_cleaned.astype (str)\n",
    "    Y_perfect = testDF['target'].astype (int)\n",
    "    X_test = transform_fv (testDF)\n",
    "    \n",
    "    predictions      = np.round (clf.predict (X_test)).astype (int)\n",
    "    testDF['target'] = predictions\n",
    "    testDF['id']     = testDF.id.astype (int)\n",
    "    testDF['target'] = testDF.target.astype (int)\n",
    "    submitDF         = testDF[['id', 'target']]\n",
    "    submitDF.to_csv ('submission_v'+str (VERSION) + clf.name + '.' + str (iterCount+1) + '.csv', index=False)\n",
    "    f1_acc = f1_score (Y_perfect, predictions)\n",
    "    print ('\\n************** F1-Score =', f1_acc, \"*************\\n\")\n",
    "    print (\"Done Prediction !\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_evaluation ():\n",
    "    \n",
    "    global evalDF\n",
    "    \n",
    "    clfs = [lsvc_1, catb_1, lsvc_2, catb_2, lsvc_3, catb_3, lsvc_4, catb_4, fcnn_1, fcnn_2, sclf_1_1, sclf_1_2, sclf_1_3, sclf_2_1]\n",
    "    accs = []\n",
    "    X_eval = transform_fv (evalDF)\n",
    "    for clf in clfs:\n",
    "        \n",
    "        print ('------------------------- evaluating:', clf.name, '-------------------------')\n",
    "        predictions = clf.predict (X_eval)\n",
    "        f1_acc = f1_score(evalDF['target_relabeled'], predictions)\n",
    "        accs.append (f1_acc)\n",
    "        print ('------------------------- '+clf.name+' f1_score =', f1_acc, '-------------------------')\n",
    "    \n",
    "    best_clf = clfs [accs.index (max (accs))]\n",
    "    print ('Best clf =', best_clf.name)\n",
    "    return best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init ()\n",
    "# fit ()\n",
    "# get_best_evaluation ()\n",
    "# fit (isFitOnOrigDF=True)\n",
    "# fit ()\n",
    "# predict (sclf_2_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main (threshold=0.9, maxIterCount=1, thresh_depreciation=0.01):\n",
    "        \n",
    "    global trainDF, X_train, BEST_CLF, TRAIN_ROW_COUNT, sclf_2_1\n",
    "\n",
    "    \n",
    "    init ()\n",
    "    fit ()\n",
    "    BEST_CLF = get_best_evaluation ()\n",
    "    newTrainAddCount = 1\n",
    "    iterCount = 0\n",
    "    while newTrainAddCount > 0 and iterCount < maxIterCount:\n",
    "        \n",
    "        threshold = threshold - iterCount * thresh_depreciation\n",
    "        if iterCount == 0:\n",
    "            fit (isFitOnOrigDF=True)\n",
    "        else:\n",
    "            fit ()\n",
    "        predict (sclf_2_1, iterCount)\n",
    "        predict (BEST_CLF, iterCount)\n",
    "        Pr = BEST_CLF.predict_proba (X_train)\n",
    "        newTrainAddCount = 0\n",
    "        for idx, row in trainDF.iterrows ():\n",
    "            if not pd.isnull (row['target_relabeled']):\n",
    "                continue\n",
    "                \n",
    "            # print ('considering idx =', idx)\n",
    "            pr = Pr[idx]\n",
    "            p  = 0\n",
    "            if pr[0] > 0.5:\n",
    "                pr = pr[0]\n",
    "                p  = 0\n",
    "            else:\n",
    "                pr = pr[1]\n",
    "                p  = 1\n",
    "            if pr >= threshold:\n",
    "                # add in the training set\n",
    "                trainDF.at[idx, 'target_relabeled'] = p\n",
    "                newTrainAddCount += 1\n",
    "        # arrange df such that 1st rows with y=0/1 then rows with y=NaN\n",
    "        arrangedDF1 = trainDF[pd.isnull (trainDF['target_relabeled']) == False]\n",
    "        arrangedDF2 = trainDF[pd.isnull (trainDF['target_relabeled'])]\n",
    "        trainDF = pd.concat ([arrangedDF1, arrangedDF2], axis=0)\n",
    "        iterCount += 1\n",
    "    trainDF.to_csv ('Data/all_'+str (VERSION) + BEST_CLF.name + '.' + str (iterCount+1)+'.csv', index=False)\n",
    "    fit ()\n",
    "    predict (sclf_2_1, iterCount)\n",
    "    predict (BEST_CLF, iterCount)\n",
    "    \n",
    "    # CV for the BEST_CLF on the original train rows\n",
    "    print ('------------------- CV for ' + BEST_CLF.name + ' on the original train rows -------------------')\n",
    "    TRAIN_ROW_COUNT += evalDF.shape[0]\n",
    "    trainDF = trainDF[:TRAIN_ROW_COUNT]\n",
    "    cv_results = []\n",
    "    cv_result = cross_validate (BEST_CLF, trainDF, cv=5)\n",
    "    cv_score = np.mean (cv_result['test_score'])\n",
    "    print ()\n",
    "    print (BEST_CLF.name, cv_score)\n",
    "    print (cv_result)\n",
    "    rmtree (CACHEDIR)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_classes (y_scores, t):\n",
    "    \"\"\"\n",
    "    This function adjusts class predictions based on the prediction threshold (t).\n",
    "    Will only work for binary classification problems.\n",
    "    \"\"\"\n",
    "    return [1 if y >= t else 0 for y in y_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold (precisions, recalls, thresholds):\n",
    "    \"\"\"\n",
    "    Modified from:\n",
    "    Hands-On Machine learning with Scikit-Learn\n",
    "    and TensorFlow; p.89\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Decision Threshold\")\n",
    "    plt.legend(loc='best')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve (fpr, tpr, label=None):\n",
    "    \"\"\"\n",
    "    The ROC curve, modified from \n",
    "    Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title('ROC Curve')\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.005, 1, 0, 1.005])\n",
    "    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "    plt.legend(loc='best')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_for_clf (clf):\n",
    "    \n",
    "    global TRAIN_ROW_COUNT, trainDF\n",
    "    print ('-------------------------- ROC for', clf.name, '---------------------------')\n",
    "    \n",
    "    # init ()\n",
    "    y_train_scores = clf.predict_proba (trainDF[:TRAIN_ROW_COUNT])[:, 1]\n",
    "    y_train = list (trainDF[:TRAIN_ROW_COUNT]['target_relabeled'])\n",
    "    \n",
    "    # ROC curve : don't understand this ?\n",
    "    fpr, tpr, auc_thresholds = roc_curve (y_train, y_train_scores)\n",
    "    print ('AUC =', auc (fpr, tpr))         # AUC of ROC\n",
    "    plot_roc_curve (fpr, tpr, 'recall_optimized')\n",
    "    \n",
    "    # plot Precisioin, Recall vs Threshold\n",
    "    p, r, thresholds = precision_recall_curve (y_train, y_train_scores)\n",
    "    plot_precision_recall_vs_threshold (p, r, thresholds)\n",
    "    \n",
    "    test_file = \"Data/test.csv\"\n",
    "    testDF = pd.read_csv (test_file)\n",
    "    # testDF = testDF[:10]\n",
    "    testDF['text'] = testDF.text.astype (str)\n",
    "    testDF['keyword'] = testDF.keyword.astype (str)\n",
    "    testDF['location'] = testDF.location.astype (str)\n",
    "    submitDF = testDF[['id', 'target']]\n",
    "    y_test_scores = clf.predict_proba (testDF)[:, 1]\n",
    "    for pr in thresholds:\n",
    "        \n",
    "        submitDF['target'] = adjusted_classes (y_test_scores, pr)\n",
    "        submitDF['id']     = submitDF.id.astype (int)\n",
    "        submitDF['target'] = submitDF.target.astype (int)\n",
    "        print (pr, sum (list (submitDF['target'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_threshold (clf, threshold_pr):\n",
    "    \n",
    "    test_file = \"Data/test.tsv\"\n",
    "    testDF = pd.read_csv (test_file, sep='\\t')\n",
    "    # testDF = testDF[:10]\n",
    "    testDF['text'] = testDF.text.astype (str)\n",
    "    testDF['keyword'] = testDF.keyword.astype (str)\n",
    "    testDF['location'] = testDF.location.astype (str)\n",
    "    Y_perfect = testDF['target'].astype (int)\n",
    "    submitDF = testDF[['id', 'target']]\n",
    "    y_test_scores = clf.predict_proba (testDF)[:, 1]\n",
    "    \n",
    "    predictions        = adjusted_classes (y_test_scores, threshold_pr)\n",
    "    submitDF['target'] = predictions\n",
    "    submitDF['id']     = submitDF.id.astype (int)\n",
    "    submitDF['target'] = submitDF.target.astype (int)\n",
    "    submitDF.to_csv ('submission_v'+str (VERSION) + clf.name + '.roc'+ str (threshold_pr) + '.csv', index=False)\n",
    "    f1_acc = f1_score (Y_perfect, predictions)\n",
    "    print ('\\n************** F1-Score =', f1_acc, \"*************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rawDF.shape = (11497, 784) trainDF.shape = (8779, 784) evalDF.shape = (2718, 784)\n",
      "fit_transform_fv () Begin with tfidf_ft.fit_transform ():\n",
      "LinearSVC with L1-based feature selection for X.shape = (5527, 128291)\n",
      "--------------------- Best parameter (CV score=0.738):\n",
      "{'classifier__C': 1, 'feature_selection__estimator__tol': 0.0001}\n",
      "New #features =  783\n",
      "LinearSVC with L1-based feature selection for X.shape = (5527, 306936)\n",
      "--------------------- Best parameter (CV score=0.721):\n",
      "{'classifier__C': 0.001, 'feature_selection__estimator__tol': 0.0001}\n",
      "New #features =  3282\n",
      "Discrete2OneHot_FeatureTransformer: fit(): X.shape = (8779, 1)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (8779, 222)\n",
      "Find K-best features based on Mutual-Info / Chi^2 for X.shape = (5527, 3515)\n",
      "--------------------- Best parameter (CV score=0.961):\n",
      "{'classifier__C': 10, 'selectKBest__k': 2812, 'selectKBest__score_func': <function mutual_info_classif at 0x000002A93E999950>}\n",
      "bert_meta_ft.fit_transform ():\n",
      "bert_meta_keyword_ica_ft.fit_transform ():\n",
      "Discrete2OneHot_FeatureTransformer: fit(): X.shape = (8779, 1)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (8779, 222)\n",
      "Kernel PCA features for X.shape = (8779, 1001)\n",
      "--------------------- Best:  parameters = ('cosine', 10) , CV = 0.7764001117845164\n",
      "KPCA dim = 1000\n",
      "ICA_FeatureTransformer: type(X), X.shape = <class 'scipy.sparse.csr.csr_matrix'> (8779, 1001)\n",
      "--------------------- Best:  parameters = (800, 1) , CV = 0.7626677214833243\n",
      "ICA dimensionality, explainedVarRaio =  800 800\n",
      "Find K-best features based on Mutual-Info / Chi^2 for X.shape = (5527, 1800)\n",
      "--------------------- Best parameter (CV score=0.740):\n",
      "{'classifier__C': 1, 'selectKBest__k': 1710, 'selectKBest__score_func': <function mutual_info_classif at 0x000002A93E999950>}\n",
      "tfidf_pca_nmf_ft.fit_transform ():\n",
      "LinearSVC with L1-based feature selection for X.shape = (5527, 306936)\n",
      "--------------------- Best parameter (CV score=0.721):\n",
      "{'classifier__C': 0.001, 'feature_selection__estimator__tol': 0.0001}\n",
      "New #features =  3282\n",
      "Discrete2OneHot_FeatureTransformer: fit(): X.shape = (8779, 1)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (8779, 222)\n",
      "Find K-best features based on Mutual-Info / Chi^2 for X.shape = (5527, 3515)\n",
      "--------------------- Best parameter (CV score=0.961):\n",
      "{'classifier__C': 10, 'selectKBest__k': 2812, 'selectKBest__score_func': <function mutual_info_classif at 0x000002A93E999950>}\n",
      "Find optimal PCA dims and the same no. of NMF features for X.shape = (8779, 2812)\n",
      "--------------------- Best:  parameters = (0.9, 0.001) , CV = 0.9514997626036367\n",
      "PCA dimensionality, explainedVarRatio =  1683 0.9\n",
      "Find K-best features based on Mutual-Info / Chi^2 for X.shape = (5527, 3366)\n",
      "--------------------- Best parameter (CV score=0.910):\n",
      "{'classifier__C': 0.005, 'selectKBest__k': 2692, 'selectKBest__score_func': <function mutual_info_classif at 0x000002A93E999950>}\n",
      "fit_transform_fv () Done.\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (2718, 222)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (2718, 222)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (2718, 222)\n",
      "fit_transform_fv () Begin with tfidf_ft.fit_transform ():\n",
      "Find K-best features based on Mutual-Info / Chi^2 for X.shape = (5527, 3515)\n",
      "--------------------- Best parameter (CV score=0.961):\n",
      "{'classifier__C': 10, 'selectKBest__k': 2812, 'selectKBest__score_func': <function mutual_info_classif at 0x000002A93E999950>}\n",
      "bert_meta_ft.fit_transform ():\n",
      "bert_meta_keyword_ica_ft.fit_transform ():\n",
      "Discrete2OneHot_FeatureTransformer: fit(): X.shape = (8779, 1)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (8779, 222)\n",
      "Kernel PCA features for X.shape = (8779, 1001)\n",
      "--------------------- Best:  parameters = ('cosine', 10) , CV = 0.7764001117845164\n",
      "KPCA dim = 1000\n",
      "ICA_FeatureTransformer: type(X), X.shape = <class 'scipy.sparse.csr.csr_matrix'> (8779, 1001)\n",
      "--------------------- Best:  parameters = (800, 1) , CV = 0.7626677214833243\n",
      "ICA dimensionality, explainedVarRaio =  800 800\n",
      "Find K-best features based on Mutual-Info / Chi^2 for X.shape = (5527, 1800)\n",
      "--------------------- Best parameter (CV score=0.740):\n",
      "{'classifier__C': 1, 'selectKBest__k': 1710, 'selectKBest__score_func': <function mutual_info_classif at 0x000002A93E999950>}\n",
      "tfidf_pca_nmf_ft.fit_transform ():\n",
      "Find K-best features based on Mutual-Info / Chi^2 for X.shape = (5527, 3366)\n",
      "--------------------- Best parameter (CV score=0.910):\n",
      "{'classifier__C': 0.005, 'selectKBest__k': 2692, 'selectKBest__score_func': <function mutual_info_classif at 0x000002A93E999950>}\n",
      "fit_transform_fv () Done.\n",
      "training LSvc for X.shape = (5527, 2812)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.964):\n",
      "{'C': 0.001}\n",
      "Done Fitting LSvc\n",
      "training LSvc for X.shape = (5527, 777)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.776):\n",
      "{'C': 0.005}\n",
      "Done Fitting LSvc\n",
      "training LSvc for X.shape = (5527, 1710)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.756):\n",
      "{'C': 1}\n",
      "Done Fitting LSvc\n",
      "training LSvc for X.shape = (5527, 2692)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.948):\n",
      "{'C': 0.001}\n",
      "Done Fitting LSvc\n",
      "training Catb for X.shape = (5527, 2812)\n",
      "Done Fitting Catb\n",
      "training Catb for X.shape = (5527, 777)\n",
      "Done Fitting Catb\n",
      "training Catb for X.shape = (5527, 1710)\n",
      "Done Fitting Catb\n",
      "training Catb for X.shape = (5527, 2692)\n",
      "Done Fitting Catb\n",
      "training FCNN for X.shape = (5527, 777)\n",
      "FCNN : Best_clf_cv_transformer: starting CV = 5\n",
      "FCNN : cv_score:   0.798\n",
      "Done Fitting FCNN\n",
      "training FCNN for X.shape = (5527, 1710)\n",
      "FCNN : Best_clf_cv_transformer: starting CV = 5\n",
      "FCNN : cv_score:   0.781\n",
      "Done Fitting FCNN\n",
      "training LSvc for X.shape = (5527, 20)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.992):\n",
      "{'C': 0.1}\n",
      "Done Fitting LSvc\n",
      "training RF for X.shape = (5527, 20)\n",
      "RF : Best_clf_cv_transformer: starting CV = 5\n",
      "RF : cv_score:   0.990\n",
      "Done Fitting RF\n",
      "training Catb for X.shape = (5527, 20)\n",
      "Done Fitting Catb\n",
      "training LSvc for X.shape = (5527, 6)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.996):\n",
      "{'C': 10}\n",
      "Done Fitting LSvc\n",
      "------------------------- evaluating: lsvc_1 -------------------------\n",
      "------------------------- lsvc_1 f1_score = 0.6936636493568367 -------------------------\n",
      "------------------------- evaluating: catb_1 -------------------------\n",
      "------------------------- catb_1 f1_score = 0.43638457109959694 -------------------------\n",
      "------------------------- evaluating: lsvc_2 -------------------------\n",
      "------------------------- lsvc_2 f1_score = 0.7729468599033816 -------------------------\n",
      "------------------------- evaluating: catb_2 -------------------------\n",
      "------------------------- catb_2 f1_score = 0.7723061921014139 -------------------------\n",
      "------------------------- evaluating: lsvc_3 -------------------------\n",
      "------------------------- lsvc_3 f1_score = 0.7542413960252061 -------------------------\n",
      "------------------------- evaluating: catb_3 -------------------------\n",
      "------------------------- catb_3 f1_score = 0.6798780487804879 -------------------------\n",
      "------------------------- evaluating: lsvc_4 -------------------------\n",
      "------------------------- lsvc_4 f1_score = 0.6692015209125476 -------------------------\n",
      "------------------------- evaluating: catb_4 -------------------------\n",
      "------------------------- catb_4 f1_score = 0.6454971369078605 -------------------------\n",
      "------------------------- evaluating: fcnn_1 -------------------------\n",
      "------------------------- fcnn_1 f1_score = 0.7572362278244631 -------------------------\n",
      "------------------------- evaluating: fcnn_2 -------------------------\n",
      "------------------------- fcnn_2 f1_score = 0.7339825730394669 -------------------------\n",
      "------------------------- evaluating: sclf_1_1 -------------------------\n",
      "------------------------- sclf_1_1 f1_score = 0.4390804597701149 -------------------------\n",
      "------------------------- evaluating: sclf_1_2 -------------------------\n",
      "------------------------- sclf_1_2 f1_score = 0.4373211219232971 -------------------------\n",
      "------------------------- evaluating: sclf_1_3 -------------------------\n",
      "------------------------- sclf_1_3 f1_score = 0.48891415577032415 -------------------------\n",
      "------------------------- evaluating: sclf_2_1 -------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- sclf_2_1 f1_score = 0.4837791690381331 -------------------------\n",
      "Best clf = lsvc_2\n",
      "fit_transform_fv () Begin with tfidf_ft.fit_transform ():\n",
      "LinearSVC with L1-based feature selection for X.shape = (8245, 161761)\n",
      "--------------------- Best parameter (CV score=0.679):\n",
      "{'classifier__C': 0.1, 'feature_selection__estimator__tol': 1e-06}\n",
      "New #features =  1039\n",
      "LinearSVC with L1-based feature selection for X.shape = (8245, 540280)\n",
      "--------------------- Best parameter (CV score=0.637):\n",
      "{'classifier__C': 0.001, 'feature_selection__estimator__tol': 0.0001}\n",
      "New #features =  4524\n",
      "Discrete2OneHot_FeatureTransformer: fit(): X.shape = (11497, 1)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (11497, 222)\n",
      "Find K-best features based on Mutual-Info / Chi^2 for X.shape = (8245, 4757)\n",
      "--------------------- Best parameter (CV score=0.954):\n",
      "{'classifier__C': 0.001, 'selectKBest__k': 4281, 'selectKBest__score_func': <function mutual_info_classif at 0x000002A93E999950>}\n",
      "bert_meta_ft.fit_transform ():\n",
      "bert_meta_keyword_ica_ft.fit_transform ():\n",
      "Discrete2OneHot_FeatureTransformer: fit(): X.shape = (11497, 1)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (11497, 222)\n",
      "Kernel PCA features for X.shape = (11497, 1001)\n",
      "--------------------- Best:  parameters = ('cosine', 10) , CV = 0.7677340135046068\n",
      "KPCA dim = 1000\n",
      "ICA_FeatureTransformer: type(X), X.shape = <class 'scipy.sparse.csr.csr_matrix'> (11497, 1001)\n",
      "--------------------- Best:  parameters = (800, 10) , CV = 0.6986353232170387\n",
      "ICA dimensionality, explainedVarRaio =  800 800\n",
      "Find K-best features based on Mutual-Info / Chi^2 for X.shape = (8245, 1800)\n",
      "--------------------- Best parameter (CV score=0.688):\n",
      "{'classifier__C': 1, 'selectKBest__k': 1710, 'selectKBest__score_func': <function mutual_info_classif at 0x000002A93E999950>}\n",
      "tfidf_pca_nmf_ft.fit_transform ():\n",
      "Find K-best features based on Mutual-Info / Chi^2 for X.shape = (8245, 4757)\n",
      "--------------------- Best parameter (CV score=0.954):\n",
      "{'classifier__C': 0.001, 'selectKBest__k': 4281, 'selectKBest__score_func': <function mutual_info_classif at 0x000002A93E999950>}\n",
      "Find optimal PCA dims and the same no. of NMF features for X.shape = (11497, 4281)\n",
      "--------------------- Best:  parameters = (0.9, 0.001) , CV = 0.8876141529628278\n",
      "PCA dimensionality, explainedVarRatio =  2486 0.9\n",
      "Find K-best features based on Mutual-Info / Chi^2 for X.shape = (8245, 4972)\n",
      "--------------------- Best parameter (CV score=0.837):\n",
      "{'classifier__C': 0.005, 'selectKBest__k': 3977, 'selectKBest__score_func': <function mutual_info_classif at 0x000002A93E999950>}\n",
      "fit_transform_fv () Done.\n",
      "training LSvc for X.shape = (8245, 4281)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.958):\n",
      "{'C': 0.001}\n",
      "Done Fitting LSvc\n",
      "training LSvc for X.shape = (8245, 777)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.772):\n",
      "{'C': 0.001}\n",
      "Done Fitting LSvc\n",
      "training LSvc for X.shape = (8245, 1710)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.699):\n",
      "{'C': 1}\n",
      "Done Fitting LSvc\n",
      "training LSvc for X.shape = (8245, 3977)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.866):\n",
      "{'C': 0.005}\n",
      "Done Fitting LSvc\n",
      "training Catb for X.shape = (8245, 4281)\n",
      "Done Fitting Catb\n",
      "training Catb for X.shape = (8245, 777)\n",
      "Done Fitting Catb\n",
      "training Catb for X.shape = (8245, 1710)\n",
      "Done Fitting Catb\n",
      "training Catb for X.shape = (8245, 3977)\n",
      "Done Fitting Catb\n",
      "training FCNN for X.shape = (8245, 777)\n",
      "FCNN : Best_clf_cv_transformer: starting CV = 5\n",
      "FCNN : cv_score:   0.791\n",
      "Done Fitting FCNN\n",
      "training FCNN for X.shape = (8245, 1710)\n",
      "FCNN : Best_clf_cv_transformer: starting CV = 5\n",
      "FCNN : cv_score:   0.730\n",
      "Done Fitting FCNN\n",
      "training LSvc for X.shape = (8245, 20)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.990):\n",
      "{'C': 0.05}\n",
      "Done Fitting LSvc\n",
      "training RF for X.shape = (8245, 20)\n",
      "RF : Best_clf_cv_transformer: starting CV = 5\n",
      "RF : cv_score:   0.989\n",
      "Done Fitting RF\n",
      "training Catb for X.shape = (8245, 20)\n",
      "Done Fitting Catb\n",
      "training LSvc for X.shape = (8245, 6)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.994):\n",
      "{'C': 0.1}\n",
      "Done Fitting LSvc\n",
      "Starting Prediction by sclf_2_1\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "\n",
      "************** F1-Score = 0.45543630424638365 *************\n",
      "\n",
      "Done Prediction !\n",
      "Starting Prediction by lsvc_2\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "\n",
      "************** F1-Score = 0.3771099217785097 *************\n",
      "\n",
      "Done Prediction !\n",
      "fit_transform_fv () Begin with tfidf_ft.fit_transform ():\n",
      "LinearSVC with L1-based feature selection for X.shape = (8448, 161761)\n",
      "--------------------- Best parameter (CV score=0.669):\n",
      "{'classifier__C': 0.1, 'feature_selection__estimator__tol': 1e-06}\n",
      "New #features =  835\n",
      "LinearSVC with L1-based feature selection for X.shape = (8448, 349030)\n",
      "--------------------- Best parameter (CV score=0.642):\n",
      "{'classifier__C': 0.001, 'feature_selection__estimator__tol': 0.0001}\n",
      "New #features =  4836\n",
      "Discrete2OneHot_FeatureTransformer: fit(): X.shape = (11497, 1)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (11497, 222)\n",
      "Find K-best features based on Mutual-Info / Chi^2 for X.shape = (8448, 5069)\n",
      "--------------------- Best parameter (CV score=0.935):\n",
      "{'classifier__C': 0.001, 'selectKBest__k': 4815, 'selectKBest__score_func': <function mutual_info_classif at 0x000002A93E999950>}\n",
      "bert_meta_ft.fit_transform ():\n",
      "bert_meta_keyword_ica_ft.fit_transform ():\n",
      "Discrete2OneHot_FeatureTransformer: fit(): X.shape = (11497, 1)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (11497, 222)\n",
      "Kernel PCA features for X.shape = (11497, 1001)\n",
      "--------------------- Best:  parameters = ('cosine', 10) , CV = 0.7618206810346629\n",
      "KPCA dim = 1000\n",
      "ICA_FeatureTransformer: type(X), X.shape = <class 'scipy.sparse.csr.csr_matrix'> (11497, 1001)\n",
      "--------------------- Best:  parameters = (800, 10) , CV = 0.6878426618742648\n",
      "ICA dimensionality, explainedVarRaio =  800 800\n",
      "Find K-best features based on Mutual-Info / Chi^2 for X.shape = (8448, 1800)\n",
      "--------------------- Best parameter (CV score=0.691):\n",
      "{'classifier__C': 1, 'selectKBest__k': 1710, 'selectKBest__score_func': <function mutual_info_classif at 0x000002A93E999950>}\n",
      "tfidf_pca_nmf_ft.fit_transform ():\n",
      "LinearSVC with L1-based feature selection for X.shape = (8448, 349030)\n",
      "--------------------- Best parameter (CV score=0.642):\n",
      "{'classifier__C': 0.001, 'feature_selection__estimator__tol': 0.0001}\n",
      "New #features =  4836\n",
      "Discrete2OneHot_FeatureTransformer: fit(): X.shape = (11497, 1)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (11497, 222)\n",
      "Find K-best features based on Mutual-Info / Chi^2 for X.shape = (8448, 5069)\n",
      "--------------------- Best parameter (CV score=0.935):\n",
      "{'classifier__C': 0.001, 'selectKBest__k': 4815, 'selectKBest__score_func': <function mutual_info_classif at 0x000002A93E999950>}\n",
      "Find optimal PCA dims and the same no. of NMF features for X.shape = (11497, 4815)\n",
      "--------------------- Best:  parameters = (0.9, 0.001) , CV = 0.8678686057352186\n",
      "PCA dimensionality, explainedVarRatio =  2658 0.9\n",
      "Find K-best features based on Mutual-Info / Chi^2 for X.shape = (8448, 5316)\n",
      "--------------------- Best parameter (CV score=0.770):\n",
      "{'classifier__C': 0.01, 'selectKBest__k': 4252, 'selectKBest__score_func': <function mutual_info_classif at 0x000002A93E999950>}\n",
      "fit_transform_fv () Done.\n",
      "training LSvc for X.shape = (8448, 4815)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.937):\n",
      "{'C': 0.1}\n",
      "Done Fitting LSvc\n",
      "training LSvc for X.shape = (8448, 777)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.765):\n",
      "{'C': 0.001}\n",
      "Done Fitting LSvc\n",
      "training LSvc for X.shape = (8448, 1710)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.724):\n",
      "{'C': 1}\n",
      "Done Fitting LSvc\n",
      "training LSvc for X.shape = (8448, 4252)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.840):\n",
      "{'C': 0.001}\n",
      "Done Fitting LSvc\n",
      "training Catb for X.shape = (8448, 4815)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Fitting Catb\n",
      "training Catb for X.shape = (8448, 777)\n",
      "Done Fitting Catb\n",
      "training Catb for X.shape = (8448, 1710)\n",
      "Done Fitting Catb\n",
      "training Catb for X.shape = (8448, 4252)\n",
      "Done Fitting Catb\n",
      "training FCNN for X.shape = (8448, 777)\n",
      "FCNN : Best_clf_cv_transformer: starting CV = 5\n",
      "FCNN : cv_score:   0.781\n",
      "Done Fitting FCNN\n",
      "training FCNN for X.shape = (8448, 1710)\n",
      "FCNN : Best_clf_cv_transformer: starting CV = 5\n",
      "FCNN : cv_score:   0.767\n",
      "Done Fitting FCNN\n",
      "training LSvc for X.shape = (8448, 20)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.989):\n",
      "{'C': 0.01}\n",
      "Done Fitting LSvc\n",
      "training RF for X.shape = (8448, 20)\n",
      "RF : Best_clf_cv_transformer: starting CV = 5\n",
      "RF : cv_score:   0.986\n",
      "Done Fitting RF\n",
      "training Catb for X.shape = (8448, 20)\n",
      "Done Fitting Catb\n",
      "training LSvc for X.shape = (8448, 6)\n",
      "LSvc : Best_clf_cv_transformer: Best parameter (CV score=0.995):\n",
      "{'C': 10}\n",
      "Done Fitting LSvc\n",
      "Starting Prediction by sclf_2_1\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "\n",
      "************** F1-Score = 0.4657407407407407 *************\n",
      "\n",
      "Done Prediction !\n",
      "Starting Prediction by lsvc_2\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "\n",
      "************** F1-Score = 0.3100046970408643 *************\n",
      "\n",
      "Done Prediction !\n",
      "------------------- CV for lsvc_2 on the original train rows -------------------\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-58ea690bcad7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.93\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxIterCount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mroc_for_clf\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mBEST_CLF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-1882868063be>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(threshold, maxIterCount, thresh_depreciation)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mcv_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcv_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcv_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mrmtree\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mCACHEDIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "main (threshold=0.93, maxIterCount=1)\n",
    "roc_for_clf (BEST_CLF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Prediction by fcnn_1\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "\n",
      "************** F1-Score = 0.7356760886172651 *************\n",
      "\n",
      "Done Prediction !\n"
     ]
    }
   ],
   "source": [
    "predict (fcnn_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Prediction by fcnn_2\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "Discrete2OneHot_FeatureTransformer: transform(): X.shape = (3263, 222)\n",
      "\n",
      "************** F1-Score = 0.7412378821774794 *************\n",
      "\n",
      "Done Prediction !\n"
     ]
    }
   ],
   "source": [
    "predict (fcnn_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_threshold (BEST_CLF, 0.9437731026789814)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_count ():\n",
    "    \n",
    "    global trainDF\n",
    "    \n",
    "    init ()\n",
    "    # get total count of data including missing data\n",
    "    total = trainDF.isnull ().sum ().sort_values (ascending=False)\n",
    "    # print (total)\n",
    "\n",
    "    # get percent of missing data relevant to all data\n",
    "    percent = (trainDF.isnull ().sum ()/trainDF.isnull ().count ()).sort_values (ascending=False)\n",
    "    # print (percent)\n",
    "    \n",
    "    missing_data = pd.concat ([total, percent], axis=1, keys=['total', 'percent'])\n",
    "    return missing_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet (tweet): \n",
    "            \n",
    "    # Special characters\n",
    "    tweet = re.sub(r\"\\x89_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89When\", \"When\", tweet)\n",
    "    tweet = re.sub(r\"\\x89\", \"\", tweet)\n",
    "    tweet = re.sub(r\"China\\x89s\", \"China's\", tweet)\n",
    "    tweet = re.sub(r\"let\\x89s\", \"let's\", tweet)\n",
    "    tweet = re.sub(r\"\\x89\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89\\x9d\", \"\", tweet)\n",
    "    tweet = re.sub(r\"_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89\", \"\", tweet)\n",
    "    tweet = re.sub(r\"fromwounds\", \"from wounds\", tweet)\n",
    "    tweet = re.sub(r\"\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\", \"\", tweet)\n",
    "    tweet = re.sub(r\"Jap_n\", \"Japan\", tweet)    \n",
    "    tweet = re.sub(r\"\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"\", \"\", tweet)\n",
    "    tweet = re.sub(r\"Suru\", \"Suruc\", tweet)\n",
    "    tweet = re.sub(r\"\", \"\", tweet)\n",
    "    tweet = re.sub(r\"3million\", \"3 million\", tweet)\n",
    "    tweet = re.sub(r\"\", \"\", tweet)\n",
    "    \n",
    "    # Contractions\n",
    "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
    "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
    "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
    "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
    "    tweet = re.sub(r\"don\\x89t\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
    "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
    "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
    "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
    "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89m\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89ve\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
    "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
    "    tweet = re.sub(r\"it\\x89s\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"doesn\\x89t\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"It\\x89s\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Here\\x89s\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89ve\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
    "    tweet = re.sub(r\"can\\x89t\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
    "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
    "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
    "    tweet = re.sub(r\"wouldn\\x89t\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
    "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
    "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
    "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
    "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
    "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
    "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
    "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
    "    tweet = re.sub(r\"That\\x89s\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
    "    tweet = re.sub(r\"You\\x89re\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
    "    tweet = re.sub(r\"Don\\x89t\", \"Do not\", tweet)\n",
    "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
    "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
    "    tweet = re.sub(r\"Can\\x89t\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89ll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89d\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
    "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
    "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
    "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
    "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
    "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
    "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
    "    tweet = re.sub(r\"dont\", \"do not\", tweet)   \n",
    "            \n",
    "    # Character entity references\n",
    "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
    "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
    "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
    "    \n",
    "    # Typos, slang and informal abbreviations\n",
    "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
    "    tweet = re.sub(r\"w/\", \"with\", tweet)\n",
    "    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n",
    "    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n",
    "    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n",
    "    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n",
    "    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n",
    "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
    "    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n",
    "    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n",
    "    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n",
    "    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n",
    "    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n",
    "    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n",
    "    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n",
    "    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n",
    "    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n",
    "    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n",
    "    \n",
    "    # Hashtags and usernames\n",
    "    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n",
    "    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n",
    "    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n",
    "    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n",
    "    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n",
    "    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n",
    "    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n",
    "    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n",
    "    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n",
    "    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n",
    "    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n",
    "    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n",
    "    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n",
    "    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n",
    "    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n",
    "    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n",
    "    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n",
    "    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n",
    "    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n",
    "    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n",
    "    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n",
    "    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n",
    "    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n",
    "    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n",
    "    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n",
    "    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n",
    "    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n",
    "    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n",
    "    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n",
    "    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n",
    "    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n",
    "    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n",
    "    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n",
    "    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n",
    "    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n",
    "    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n",
    "    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n",
    "    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n",
    "    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n",
    "    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n",
    "    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n",
    "    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n",
    "    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n",
    "    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n",
    "    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n",
    "    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n",
    "    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n",
    "    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n",
    "    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n",
    "    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n",
    "    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n",
    "    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n",
    "    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n",
    "    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n",
    "    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n",
    "    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n",
    "    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n",
    "    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n",
    "    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n",
    "    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n",
    "    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n",
    "    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n",
    "    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n",
    "    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n",
    "    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n",
    "    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n",
    "    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n",
    "    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n",
    "    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n",
    "    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n",
    "    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n",
    "    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n",
    "    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n",
    "    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\n",
    "    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n",
    "    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n",
    "    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n",
    "    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n",
    "    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n",
    "    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n",
    "    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n",
    "    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n",
    "    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n",
    "    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n",
    "    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n",
    "    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n",
    "    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n",
    "    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n",
    "    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\n",
    "    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\n",
    "    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n",
    "    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n",
    "    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n",
    "    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n",
    "    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n",
    "    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\n",
    "    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n",
    "    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n",
    "    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n",
    "    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n",
    "    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n",
    "    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n",
    "    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n",
    "    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n",
    "    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n",
    "    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n",
    "    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n",
    "    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n",
    "    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n",
    "    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n",
    "    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n",
    "    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n",
    "    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n",
    "    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n",
    "    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n",
    "    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n",
    "    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n",
    "    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n",
    "    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n",
    "    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\n",
    "    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n",
    "    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n",
    "    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n",
    "    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n",
    "    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n",
    "    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n",
    "    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n",
    "    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\n",
    "    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\n",
    "    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\n",
    "    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\n",
    "    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\n",
    "    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\n",
    "    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n",
    "    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n",
    "    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n",
    "    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\n",
    "    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\n",
    "    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\n",
    "    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\n",
    "    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\n",
    "    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\n",
    "    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\n",
    "    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\n",
    "    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\n",
    "    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\n",
    "    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\n",
    "    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\n",
    "    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\n",
    "    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\n",
    "    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\n",
    "    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\n",
    "    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\n",
    "    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\n",
    "    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\n",
    "    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\n",
    "    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\n",
    "    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\n",
    "    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\n",
    "    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\n",
    "    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n",
    "    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\n",
    "    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\n",
    "    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\n",
    "    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\n",
    "    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\n",
    "    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n",
    "    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\n",
    "    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\n",
    "    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\n",
    "    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\n",
    "    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n",
    "    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n",
    "    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\n",
    "    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\n",
    "    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\n",
    "    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\n",
    "    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\n",
    "    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\n",
    "    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\n",
    "    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\n",
    "    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\n",
    "    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\n",
    "    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\n",
    "    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\n",
    "    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\n",
    "    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\n",
    "    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\n",
    "    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\n",
    "    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\n",
    "    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\n",
    "    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\n",
    "    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\n",
    "    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\n",
    "    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n",
    "    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\n",
    "    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\n",
    "    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\n",
    "    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\n",
    "    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n",
    "    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\n",
    "    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n",
    "    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n",
    "    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n",
    "    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\n",
    "    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\n",
    "    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\n",
    "    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\n",
    "    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\n",
    "    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\n",
    "    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\n",
    "    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\n",
    "    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n",
    "    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\n",
    "    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\n",
    "    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\n",
    "    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n",
    "    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\n",
    "    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\n",
    "    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\n",
    "    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\n",
    "    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\n",
    "    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\n",
    "    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\n",
    "    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n",
    "    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\n",
    "    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\n",
    "    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n",
    "    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\n",
    "    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n",
    "    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n",
    "    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n",
    "    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n",
    "    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\n",
    "    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\n",
    "    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\n",
    "    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\n",
    "    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\n",
    "    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\n",
    "    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\n",
    "    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\n",
    "    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\n",
    "    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\n",
    "    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\n",
    "    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n",
    "    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n",
    "    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\n",
    "    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\n",
    "    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\n",
    "    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n",
    "    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n",
    "    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\n",
    "    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\n",
    "    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\n",
    "    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\n",
    "    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n",
    "    tweet = re.sub(r\"Newss\", \"News\", tweet)\n",
    "    tweet = re.sub(r\"hempoil\", \"hemp oil\", tweet)\n",
    "    tweet = re.sub(r\"CommoditiesAre\", \"Commodities are\", tweet)\n",
    "    tweet = re.sub(r\"tubestrike\", \"tube strike\", tweet)\n",
    "    tweet = re.sub(r\"JoeNBC\", \"Joe Scarborough\", tweet)\n",
    "    tweet = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", tweet)\n",
    "    tweet = re.sub(r\"TI5\", \"The International 5\", tweet)\n",
    "    tweet = re.sub(r\"thehill\", \"the hill\", tweet)\n",
    "    tweet = re.sub(r\"3others\", \"3 others\", tweet)\n",
    "    tweet = re.sub(r\"stighefootball\", \"Sam Tighe\", tweet)\n",
    "    tweet = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", tweet)\n",
    "    tweet = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", tweet)\n",
    "    tweet = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", tweet)\n",
    "    tweet = re.sub(r\"carsonmwr\", \"Fort Carson\", tweet)\n",
    "    tweet = re.sub(r\"offdishduty\", \"off dish duty\", tweet)\n",
    "    tweet = re.sub(r\"andword\", \"and word\", tweet)\n",
    "    tweet = re.sub(r\"rhodeisland\", \"Rhode Island\", tweet)\n",
    "    tweet = re.sub(r\"easternoregon\", \"Eastern Oregon\", tweet)\n",
    "    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n",
    "    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n",
    "    tweet = re.sub(r\"57am\", \"57 am\", tweet)\n",
    "    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n",
    "    tweet = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", tweet)\n",
    "    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\n",
    "    tweet = re.sub(r\"under50\", \"under 50\", tweet)\n",
    "    tweet = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", tweet)\n",
    "    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\n",
    "    tweet = re.sub(r\"amwriting\", \"am writing\", tweet)\n",
    "    tweet = re.sub(r\"Bokoharm\", \"Boko Haram\", tweet)\n",
    "    tweet = re.sub(r\"Nowlike\", \"Now like\", tweet)\n",
    "    tweet = re.sub(r\"seasonfrom\", \"season from\", tweet)\n",
    "    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n",
    "    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n",
    "    tweet = re.sub(r\"sicklife\", \"sick life\", tweet)\n",
    "    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\n",
    "    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\n",
    "    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\n",
    "    tweet = re.sub(r\"evng\", \"evening\", tweet)\n",
    "    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\n",
    "    tweet = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", tweet)\n",
    "    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\n",
    "    tweet = re.sub(r\"ABCNetwork\", \"ABC Network\", tweet)\n",
    "    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\n",
    "    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\n",
    "    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\n",
    "    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\n",
    "    tweet = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", tweet)\n",
    "    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\n",
    "    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\n",
    "    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\n",
    "    tweet = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", tweet)\n",
    "    tweet = re.sub(r\"Beingtweets\", \"Being tweets\", tweet)\n",
    "    tweet = re.sub(r\"newauthors\", \"new authors\", tweet)\n",
    "    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\n",
    "    tweet = re.sub(r\"44PM\", \"44 PM\", tweet)\n",
    "    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\n",
    "    tweet = re.sub(r\"40PM\", \"40 PM\", tweet)\n",
    "    tweet = re.sub(r\"myswc\", \"Severe Weather Center\", tweet)\n",
    "    tweet = re.sub(r\"ithats\", \"that is\", tweet)\n",
    "    tweet = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", tweet)\n",
    "    tweet = re.sub(r\"FatLoss\", \"Fat Loss\", tweet)\n",
    "    tweet = re.sub(r\"02PM\", \"02 PM\", tweet)\n",
    "    tweet = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", tweet)\n",
    "    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\n",
    "    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\n",
    "    tweet = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", tweet)\n",
    "    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n",
    "    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\n",
    "    tweet = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", tweet)\n",
    "    tweet = re.sub(r\"GeorgeTakei\", \"George Takei\", tweet)\n",
    "    tweet = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", tweet)\n",
    "    tweet = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", tweet)\n",
    "    tweet = re.sub(r\"incubusband\", \"incubus band\", tweet)\n",
    "    tweet = re.sub(r\"Babypicturethis\", \"Baby picture this\", tweet)\n",
    "    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\n",
    "    tweet = re.sub(r\"win10\", \"Windows 10\", tweet)\n",
    "    tweet = re.sub(r\"idkidk\", \"I do not know I do not know\", tweet)\n",
    "    tweet = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", tweet)\n",
    "    tweet = re.sub(r\"amyschumer\", \"Amy Schumer\", tweet)\n",
    "    tweet = re.sub(r\"crewlist\", \"crew list\", tweet)\n",
    "    tweet = re.sub(r\"Erdogans\", \"Erdogan\", tweet)\n",
    "    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\n",
    "    tweet = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", tweet)\n",
    "    tweet = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", tweet)\n",
    "    tweet = re.sub(r\"georgegallagher\", \"George Gallagher\", tweet)\n",
    "    tweet = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", tweet)\n",
    "    tweet = re.sub(r\"pctool\", \"pc tool\", tweet)\n",
    "    tweet = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", tweet)\n",
    "    tweet = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", tweet)\n",
    "    tweet = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", tweet)\n",
    "    tweet = re.sub(r\"LakeEffect\", \"Lake Effect\", tweet)\n",
    "    tweet = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", tweet)\n",
    "    tweet = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", tweet)\n",
    "    tweet = re.sub(r\"writerslife\", \"writers life\", tweet)\n",
    "    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\n",
    "    tweet = re.sub(r\"UnusualWords\", \"Unusual Words\", tweet)\n",
    "    tweet = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", tweet)\n",
    "    tweet = re.sub(r\"acreativedc\", \"a creative DC\", tweet)\n",
    "    tweet = re.sub(r\"vscodc\", \"vsco DC\", tweet)\n",
    "    tweet = re.sub(r\"VSCOcam\", \"vsco camera\", tweet)\n",
    "    tweet = re.sub(r\"TheBEACHDC\", \"The beach DC\", tweet)\n",
    "    tweet = re.sub(r\"buildingmuseum\", \"building museum\", tweet)\n",
    "    tweet = re.sub(r\"WorldOil\", \"World Oil\", tweet)\n",
    "    tweet = re.sub(r\"redwedding\", \"red wedding\", tweet)\n",
    "    tweet = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", tweet)\n",
    "    tweet = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", tweet)\n",
    "    tweet = re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", tweet)\n",
    "    tweet = re.sub(r\"bleased\", \"blessed\", tweet)\n",
    "    tweet = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", tweet)\n",
    "    tweet = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", tweet)\n",
    "    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\n",
    "    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\n",
    "    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\n",
    "    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\n",
    "    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\n",
    "    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\n",
    "    tweet = re.sub(r\"changetheworld\", \"change the world\", tweet)\n",
    "    tweet = re.sub(r\"Ebolacase\", \"Ebola case\", tweet)\n",
    "    tweet = re.sub(r\"mcgtech\", \"mcg technologies\", tweet)\n",
    "    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n",
    "    tweet = re.sub(r\"advancedwarfare\", \"advanced warfare\", tweet)\n",
    "    tweet = re.sub(r\"letsFootball\", \"let us Football\", tweet)\n",
    "    tweet = re.sub(r\"LateNiteMix\", \"late night mix\", tweet)\n",
    "    tweet = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", tweet)\n",
    "    tweet = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", tweet)\n",
    "    tweet = re.sub(r\"22PM\", \"22 PM\", tweet)\n",
    "    tweet = re.sub(r\"54am\", \"54 AM\", tweet)\n",
    "    tweet = re.sub(r\"38am\", \"38 AM\", tweet)\n",
    "    tweet = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", tweet)\n",
    "    tweet = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", tweet)\n",
    "    tweet = re.sub(r\"InsaneLimits\", \"Insane Limits\", tweet)\n",
    "    tweet = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", tweet)\n",
    "    tweet = re.sub(r\"2k15\", \"2015\", tweet)\n",
    "    tweet = re.sub(r\"TheIran\", \"Iran\", tweet)\n",
    "    tweet = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", tweet)\n",
    "    tweet = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", tweet)\n",
    "    tweet = re.sub(r\"defense_news\", \"defense news\", tweet)\n",
    "    tweet = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", tweet)\n",
    "    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\n",
    "    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n",
    "    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n",
    "    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\n",
    "    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\n",
    "    tweet = re.sub(r\"ProBonoNews\", \"Pro Bono News\", tweet)\n",
    "    tweet = re.sub(r\"JakartaPost\", \"Jakarta Post\", tweet)\n",
    "    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\n",
    "    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\n",
    "    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\n",
    "    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\n",
    "    tweet = re.sub(r\"ineedcake\", \"I need cake\", tweet)\n",
    "    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\n",
    "    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\n",
    "    tweet = re.sub(r\"AlexxPage\", \"Alex Page\", tweet)\n",
    "    tweet = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", tweet)\n",
    "    tweet = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", tweet)\n",
    "    tweet = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", tweet)\n",
    "    tweet = re.sub(r\"irongiant\", \"iron giant\", tweet)\n",
    "    tweet = re.sub(r\"RonFunches\", \"Ron Funches\", tweet)\n",
    "    tweet = re.sub(r\"TimCook\", \"Tim Cook\", tweet)\n",
    "    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\n",
    "    tweet = re.sub(r\"Madsummer\", \"Mad summer\", tweet)\n",
    "    tweet = re.sub(r\"NowYouKnow\", \"Now you know\", tweet)\n",
    "    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\n",
    "    tweet = re.sub(r\"TomLandry\", \"Tom Landry\", tweet)\n",
    "    tweet = re.sub(r\"showgirldayoff\", \"show girl day off\", tweet)\n",
    "    tweet = re.sub(r\"Yougslavia\", \"Yugoslavia\", tweet)\n",
    "    tweet = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", tweet)\n",
    "    tweet = re.sub(r\"FromTheDesk\", \"From The Desk\", tweet)\n",
    "    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\n",
    "    tweet = re.sub(r\"CatoInstitute\", \"Cato Institute\", tweet)\n",
    "    tweet = re.sub(r\"EmekaGift\", \"Emeka Gift\", tweet)\n",
    "    tweet = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", tweet)\n",
    "    tweet = re.sub(r\"Cynicalreality\", \"Cynical reality\", tweet)\n",
    "    tweet = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", tweet)\n",
    "    tweet = re.sub(r\"NotSorry\", \"not sorry\", tweet)\n",
    "    tweet = re.sub(r\"UseYourWords\", \"use your words\", tweet)\n",
    "    tweet = re.sub(r\"WordoftheDay\", \"word of the day\", tweet)\n",
    "    tweet = re.sub(r\"Dictionarycom\", \"Dictionary.com\", tweet)\n",
    "    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\n",
    "    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\n",
    "    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\n",
    "    tweet = re.sub(r\"uiseful\", \"useful\", tweet)\n",
    "    tweet = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", tweet)\n",
    "    tweet = re.sub(r\"autoaccidents\", \"auto accidents\", tweet)\n",
    "    tweet = re.sub(r\"SteveGursten\", \"Steve Gursten\", tweet)\n",
    "    tweet = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", tweet)\n",
    "    tweet = re.sub(r\"birdgang\", \"bird gang\", tweet)\n",
    "    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\n",
    "    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\n",
    "    tweet = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", tweet)\n",
    "    tweet = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", tweet)\n",
    "    tweet = re.sub(r\"david_brelsford\", \"David Brelsford\", tweet)\n",
    "    tweet = re.sub(r\"TOI_India\", \"The Times of India\", tweet)\n",
    "    tweet = re.sub(r\"hegot\", \"he got\", tweet)\n",
    "    tweet = re.sub(r\"SkinsOn9\", \"Skins on 9\", tweet)\n",
    "    tweet = re.sub(r\"sothathappened\", \"so that happened\", tweet)\n",
    "    tweet = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", tweet)\n",
    "    tweet = re.sub(r\"NationFirst\", \"Nation First\", tweet)\n",
    "    tweet = re.sub(r\"IndiaToday\", \"India Today\", tweet)\n",
    "    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\n",
    "    tweet = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", tweet)\n",
    "    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\n",
    "    tweet = re.sub(r\"BidTime\", \"Bid Time\", tweet)\n",
    "    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\n",
    "    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\n",
    "    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\n",
    "    tweet = re.sub(r\"eatshit\", \"eat shit\", tweet)\n",
    "    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\n",
    "    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\n",
    "    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\n",
    "    tweet = re.sub(r\"facilitiesmanagement\", \"facilities management\", tweet)\n",
    "    tweet = re.sub(r\"facilitydude\", \"facility dude\", tweet)\n",
    "    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\n",
    "    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\n",
    "    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\n",
    "    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\n",
    "    tweet = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", tweet)\n",
    "    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\n",
    "    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\n",
    "    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\n",
    "    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\n",
    "    tweet = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", tweet)\n",
    "    tweet = re.sub(r\"RiverComplex\", \"River Complex\", tweet)\n",
    "    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\n",
    "    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\n",
    "    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\n",
    "    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\n",
    "    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\n",
    "    tweet = re.sub(r\"slownewsday\", \"slow news day\", tweet)\n",
    "    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n",
    "    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\n",
    "    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\n",
    "    tweet = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", tweet)\n",
    "    tweet = re.sub(r\"netbots\", \"net bots\", tweet)\n",
    "    tweet = re.sub(r\"byebyeroad\", \"bye bye road\", tweet)\n",
    "    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\n",
    "    tweet = re.sub(r\"EndofUS\", \"End of United States\", tweet)\n",
    "    tweet = re.sub(r\"35PM\", \"35 PM\", tweet)\n",
    "    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\n",
    "    tweet = re.sub(r\"76mins\", \"76 minutes\", tweet)\n",
    "    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\n",
    "    tweet = re.sub(r\"livesmatter\", \"lives matter\", tweet)\n",
    "    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\n",
    "    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\n",
    "    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\n",
    "    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\n",
    "    tweet = re.sub(r\"instaxbooty\", \"instagram booty\", tweet)\n",
    "    tweet = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", tweet)\n",
    "    tweet = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", tweet)\n",
    "    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\n",
    "    tweet = re.sub(r\"OculusRift\", \"Oculus Rift\", tweet)\n",
    "    tweet = re.sub(r\"OwenJones84\", \"Owen Jones\", tweet)\n",
    "    tweet = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", tweet)\n",
    "    tweet = re.sub(r\"paulrogers002\", \"Paul Rogers\", tweet)\n",
    "    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\n",
    "    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\n",
    "    tweet = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", tweet)\n",
    "    tweet = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", tweet)\n",
    "    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\n",
    "    tweet = re.sub(r\"YEEESSSS\", \"yes\", tweet)\n",
    "    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\n",
    "    tweet = re.sub(r\"IntlDevelopment\", \"Intl Development\", tweet)\n",
    "    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n",
    "    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\n",
    "    tweet = re.sub(r\"NewsThousands\", \"News Thousands\", tweet)\n",
    "    tweet = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", tweet)\n",
    "    tweet = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", tweet)\n",
    "    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\n",
    "    tweet = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", tweet)\n",
    "    tweet = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", tweet)\n",
    "    tweet = re.sub(r\"Alltheway80s\", \"All the way 80s\", tweet)\n",
    "    tweet = re.sub(r\"FromTheField\", \"From the field\", tweet)\n",
    "    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\n",
    "    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\n",
    "    tweet = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", tweet)\n",
    "    tweet = re.sub(r\"feelingmanly\", \"feeling manly\", tweet)\n",
    "    tweet = re.sub(r\"stillnotoverit\", \"still not over it\", tweet)\n",
    "    tweet = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", tweet)\n",
    "    tweet = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", tweet)\n",
    "    tweet = re.sub(r\"ServicesGold\", \"Services Gold\", tweet)\n",
    "    tweet = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", tweet)\n",
    "    tweet = re.sub(r\"Evaucation\", \"evacuation\", tweet)\n",
    "    tweet = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", tweet)\n",
    "    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\n",
    "    tweet = re.sub(r\"Tubestrike\", \"tube strike\", tweet)\n",
    "    tweet = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", tweet)\n",
    "    tweet = re.sub(r\"localplumber\", \"local plumber\", tweet)\n",
    "    tweet = re.sub(r\"awesomejobsiri\", \"awesome job siri\", tweet)\n",
    "    tweet = re.sub(r\"PayForItHow\", \"Pay for it how\", tweet)\n",
    "    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\n",
    "    tweet = re.sub(r\"crimeairnetwork\", \"crime air network\", tweet)\n",
    "    tweet = re.sub(r\"KimAcheson\", \"Kim Acheson\", tweet)\n",
    "    tweet = re.sub(r\"cityofcalgary\", \"City of Calgary\", tweet)\n",
    "    tweet = re.sub(r\"prosyndicate\", \"pro syndicate\", tweet)\n",
    "    tweet = re.sub(r\"660NEWS\", \"660 NEWS\", tweet)\n",
    "    tweet = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", tweet)\n",
    "    tweet = re.sub(r\"wfocus\", \"focus\", tweet)\n",
    "    tweet = re.sub(r\"ShastaDam\", \"Shasta Dam\", tweet)\n",
    "    tweet = re.sub(r\"go2MarkFranco\", \"Mark Franco\", tweet)\n",
    "    tweet = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", tweet)\n",
    "    tweet = re.sub(r\"Nashgrier\", \"Nash Grier\", tweet)\n",
    "    tweet = re.sub(r\"NashNewVideo\", \"Nash new video\", tweet)\n",
    "    tweet = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", tweet)\n",
    "    tweet = re.sub(r\"SHGames\", \"Sledgehammer Games\", tweet)\n",
    "    tweet = re.sub(r\"bedhair\", \"bed hair\", tweet)\n",
    "    tweet = re.sub(r\"JoelHeyman\", \"Joel Heyman\", tweet)\n",
    "    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\n",
    "           \n",
    "    # Urls\n",
    "    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n",
    "        \n",
    "    # Words with punctuations and special characters\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
    "    for p in punctuations:\n",
    "        tweet = tweet.replace(p, f' {p} ')\n",
    "        \n",
    "    # ... and ..\n",
    "    tweet = tweet.replace('...', ' ... ')\n",
    "    if '...' not in tweet:\n",
    "        tweet = tweet.replace('..', ' ... ')      \n",
    "        \n",
    "    # Acronyms\n",
    "    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n",
    "    tweet = re.sub(r\"msica\", \"music\", tweet)\n",
    "    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n",
    "    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n",
    "    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n",
    "    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n",
    "    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n",
    "    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n",
    "    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n",
    "    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n",
    "    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n",
    "    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n",
    "    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \n",
    "    \n",
    "    # Grouping same words without embeddings\n",
    "    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n",
    "    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n",
    "    return tweet\n",
    "\n",
    "# clean file\n",
    "def cleanDFs ():\n",
    "    \n",
    "    global TRAIN_ROW_COUNT\n",
    "    \n",
    "    text_cleaner_transformer = Text_cleaner_transformer ()\n",
    "    files = [\"Data/test.tsv\"]  # [\"Data/all.tsv\", \"Data/train.tsv\", \"Data/test.tsv\"]\n",
    "    for file in files:\n",
    "        \n",
    "        df = pd.read_csv (file, sep='\\t')\n",
    "        df['text_cleaned'] = df['text'].apply(lambda s : cleanTweet (s))\n",
    "        if file != 'Data/test.tsv':\n",
    "            \n",
    "            df['target_relabeled'] = df['target'].copy() \n",
    "            df.loc[df['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target_relabeled'] = 0\n",
    "            df.loc[df['text'] == 'Hellfire is surrounded by desires so be careful and dont let your desires control you! #Afterlife', 'target_relabeled'] = 0\n",
    "            df.loc[df['text'] == 'To fight bioterrorism sir.', 'target_relabeled'] = 0\n",
    "            df.loc[df['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4', 'target_relabeled'] = 1\n",
    "            df.loc[df['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring', 'target_relabeled'] = 1\n",
    "            df.loc[df['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target_relabeled'] = 0\n",
    "            df.loc[df['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target_relabeled'] = 0\n",
    "            df.loc[df['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target_relabeled'] = 1\n",
    "            df.loc[df['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG', 'target_relabeled'] = 1\n",
    "            df.loc[df['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target_relabeled'] = 0\n",
    "            df.loc[df['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target_relabeled'] = 0\n",
    "            df.loc[df['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target_relabeled'] = 0\n",
    "            df.loc[df['text'] == \"Hellfire! We dont even want to think about it or mention it so lets not do anything that leads to it #islam!\", 'target_relabeled'] = 0\n",
    "            df.loc[df['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target_relabeled'] = 0\n",
    "            df.loc[df['text'] == \"Caution: breathing may be hazardous to your health.\", 'target_relabeled'] = 1\n",
    "            df.loc[df['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target_relabeled'] = 0\n",
    "            df.loc[df['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target_relabeled'] = 0\n",
    "            df.loc[df['text'] == \"that horrible sinking feeling when youve been at home on your phone for a while and you realise its been on 3G this whole time\", 'target_relabeled'] = 0\n",
    "        \n",
    "        if file == 'Data/test.tsv':\n",
    "                df['target_relabeled'] = df['target']\n",
    "        arrangedDF1 = df[pd.isnull (df['target_relabeled']) == False]\n",
    "        arrangedDF2 = df[pd.isnull (df['target_relabeled'])]\n",
    "        df  = pd.concat ([arrangedDF1, arrangedDF2], axis=0)\n",
    "        df.to_csv (file, index=False, sep='\\t')\n",
    "        if file == \"Data/all.tsv\":\n",
    "            TRAIN_ROW_COUNT = arrangedDF1.shape[0]\n",
    "            print ('TRAIN_ROW_COUNT =', TRAIN_ROW_COUNT)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = \"Data/train.tsv\"\n",
    "# df = pd.read_csv (file, sep='\\t')\n",
    "# df['target_relabeled'] = df['target'].copy()\n",
    "# arrangedDF1 = df[pd.isnull (df['target_relabeled']) == False]\n",
    "# arrangedDF2 = df[pd.isnull (df['target_relabeled'])]\n",
    "# df  = pd.concat ([arrangedDF1, arrangedDF2], axis=0)\n",
    "# df.to_csv (file, index=False, sep='\\t')\n",
    "# if file == \"Data/all.tsv\":\n",
    "#     TRAIN_ROW_COUNT = arrangedDF1.shape[0]\n",
    "# print ('TRAIN_ROW_COUNT =', TRAIN_ROW_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_meta_features ():\n",
    "    \n",
    "    files = [\"Data/all.csv\", \"Data/train.csv\", \"Data/test.csv\"]\n",
    "    for file in files:\n",
    "        \n",
    "        df = pd.read_csv (file)\n",
    "        \n",
    "        # word_count\n",
    "        df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "        # unique_word_count\n",
    "        df['unique_word_count'] = df['text'].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "        # stop_word_count\n",
    "        df['stop_word_count'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "\n",
    "        # url_count\n",
    "        df['url_count'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "\n",
    "        # mean_word_length\n",
    "        df['mean_word_length'] = df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "        # char_count\n",
    "        df['char_count'] = df['text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "        # punctuation_count\n",
    "        df['punctuation_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "        # hashtag_count\n",
    "        df['hashtag_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "\n",
    "        # mention_count\n",
    "        df['mention_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "        \n",
    "        file = os.path.splitext (file)[0] + '.tsv'\n",
    "        df.to_csv (file, index=False, sep='\\t')\n",
    "        print ('Done MetaInfo for', file)\n",
    "    return\n",
    "   \n",
    "def add_bert_features ():\n",
    "    \n",
    "    files = [\"Data/test.tsv\"]   # [\"Data/all.tsv\", \"Data/train.tsv\", \"Data/test.tsv\"]\n",
    "    for file in files:\n",
    "        \n",
    "        print ('Starting Bert for', file)\n",
    "        df = pd.read_csv (file, sep='\\t')\n",
    "        # bert\n",
    "        bc = BertClient ()\n",
    "        bertMatrix = bc.encode (list(df['text_cleaned']))\n",
    "        bertDF = pd.DataFrame (data=bertMatrix, columns=['bert_'+str (i) for i in range (bertMatrix.shape[1])])\n",
    "        df = pd.concat ((df, bertDF), axis=1)\n",
    "        df.to_csv (file, index=False, sep='\\t')\n",
    "        print ('Done Bert for', file)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_meta_features ()\n",
    "# cleanDFs ()\n",
    "# add_bert_features ()\n",
    "# main (threshold=0.9, maxIterCount=1)\n",
    "# roc_for_clf (BEST_CLF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "class MyModule (nn.Module):\n",
    "    \n",
    "    def __init__(self, inputCount=1024, outputCount=2, hiddenLayerCounts=[128], drop_prob=0.2, nonlin=nn.ReLU ()):\n",
    "        \n",
    "        super(MyModule, self).__init__()\n",
    "\n",
    "        self.nonlin  = nonlin\n",
    "        self.dropout = nn.Dropout (drop_prob)\n",
    "        \n",
    "        self.dense1     = nn.Linear (inputCount, hiddenLayerCounts[0])\n",
    "        self.batchnorm1 = nn.BatchNorm1d (hiddenLayerCounts[0])\n",
    "        # self.dense2     = nn.Linear(hiddenLayerCounts[0], hiddenLayerCounts[1])\n",
    "        # self.batchnorm2 = nn.BatchNorm1d (hiddenLayerCounts[1])\n",
    "        # self.dense3     = nn.Linear(hiddenLayerCounts[1], hiddenLayerCounts[2])\n",
    "        # self.batchnorm3 = nn.BatchNorm1d (hiddenLayerCounts[2])        \n",
    "        self.outDense   = nn.Linear (hiddenLayerCounts[-1], outputCount)\n",
    "        \n",
    "        self.outActivtn = None\n",
    "        if outputCount == 1 or outputCount == 2:\n",
    "            self.outActivtn = nn.Sigmoid ()\n",
    "        else:\n",
    "            self.outActivtn = nn.Softmax (dim=-1)\n",
    "        return\n",
    "\n",
    "    def forward (self, X, **kwargs):\n",
    "        \n",
    "        X = self.dropout (self.nonlin (self.batchnorm1 (self.dense1 (X))))\n",
    "        # X = self.dropout (self.nonlin (self.batchnorm2 (self.dense2 (X))))\n",
    "        # X = self.dropout (self.nonlin (batchnorm3 (self.dense3 (X))))\n",
    "        X = self.outActivtn (self.outDense (X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT = bert_meta_keyword_ica_ft\n",
    "\n",
    "def get_X_for_FCNN ():\n",
    "    \n",
    "    global trainDF, evalDF, TRAIN_ROW_COUNT, bert_meta_ft, FT\n",
    "    TRAIN_ROW_COUNT = 8234\n",
    "    \n",
    "    train_file = \"Data/all.tsv\"\n",
    "    rawDF = pd.read_csv (train_file, sep='\\t')  #, dtype={'id': np.int16, 'target_relabeled': np.int8})\n",
    "    # rawDF = rawDF[0:30]               # Comment this out ********************************\n",
    "    \n",
    "    labeledDF = rawDF[:TRAIN_ROW_COUNT]\n",
    "    trainDF, evalDF = train_test_split (labeledDF, test_size=0.33, random_state=42)\n",
    "    trainDF_temp = pd.concat ((trainDF, rawDF[TRAIN_ROW_COUNT:]), axis=0)\n",
    "    TRAIN_ROW_COUNT = trainDF.shape[0]\n",
    "    trainDF = trainDF_temp\n",
    "    \n",
    "    print ('rawDF.shape =', rawDF.shape, 'trainDF.shape =', trainDF.shape, 'evalDF.shape =', evalDF.shape)\n",
    "    rawDF, labeledDF, trainDF_temp = None, None, None\n",
    "    \n",
    "    X = FT.fit_transform (trainDF, trainDF['target_relabeled'])\n",
    "    Y = trainDF['target_relabeled']\n",
    "    X = X[:TRAIN_ROW_COUNT, :]\n",
    "    Y = Y[:TRAIN_ROW_COUNT]    \n",
    "    X = X.astype (np.float32)\n",
    "    Y = Y.astype (np.int64)\n",
    "    print ('X.shape =', X.shape)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tune_FCNN ():\n",
    "    \n",
    "    global FT\n",
    "    \n",
    "    X, Y = get_X_for_FCNN ()\n",
    "    CV_SCORERS = {\n",
    "        'precision_score':         make_scorer (precision_score),\n",
    "        'recall_score':            make_scorer (recall_score),\n",
    "        'f1_score':                make_scorer (f1_score),\n",
    "        'balanced_accuracy_score': make_scorer (balanced_accuracy_score)\n",
    "    }\n",
    "    cv_scores = []\n",
    "    balanced_accs = []\n",
    "    param_grid = {\n",
    "        # 'lr': [0.01, 0.005],\n",
    "        # 'max_epochs': [30, 80],\n",
    "        'module__hiddenLayerCounts': [[5], [10], [20]]\n",
    "    }\n",
    "    lrScheduler = LRScheduler (\n",
    "        CyclicLR,\n",
    "        base_lr=0.0001,\n",
    "        max_lr=0.05,\n",
    "        step_every='batch'\n",
    "    )\n",
    "    clf = NeuralNetClassifier (\n",
    "\n",
    "            module=MyModule,\n",
    "            module__inputCount=X.shape[1], \n",
    "            module__outputCount=2, \n",
    "            # module__hiddenLayerCounts=[hidden_count],\n",
    "            max_epochs=1000,\n",
    "            # lr=0.01,\n",
    "            iterator_train__shuffle=True,\n",
    "            callbacks=[('LRScheduler', lrScheduler), ('EarlyStopping', EarlyStopping (patience=20))]\n",
    "    )\n",
    "    gridSearchCV = GridSearchCV (\n",
    "        clf, param_grid=param_grid, iid=False, cv=4, scoring=CV_SCORERS, \n",
    "        refit='f1_score'\n",
    "    )\n",
    "    gridSearchCV.fit (X, Y)\n",
    "    clf = gridSearchCV.best_estimator_\n",
    "    cv_score = gridSearchCV.best_score_ \n",
    "    print ('Best CV params =', gridSearchCV.best_params_)\n",
    "    hiddenLayerCounts = gridSearchCV.best_params_['module__hiddenLayerCounts']\n",
    "    # clf.fit (X, Y) # not required\n",
    "\n",
    "    X_eval = FT.transform (evalDF)\n",
    "    X_eval = X_eval.astype (np.float32)\n",
    "    Y_eval = evalDF['target_relabeled']\n",
    "    Y_eval = Y_eval.astype (np.int64)\n",
    "    predictions = clf.predict (X_eval).astype (np.int64)\n",
    "    f1_acc = f1_score(Y_eval, predictions)\n",
    "    print ('------------------------- cv_score =', cv_score, 'f1_score =', f1_acc, '-------------------------')\n",
    "\n",
    "    # plt.plot (hidden_counts, cv_scores, 'bs', hidden_counts, balanced_accs, 'g^')\n",
    "    # plt.show ()\n",
    "    return hiddenLayerCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_LayerCounts = 10\n",
    "# HIDDEN_LayerCounts = tune_FCNN ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 iterations, lr=0.01:\n",
    "\n",
    "bert + meta:\n",
    "    5 : cv_score = 0.7493871012282531 f1_score = 0.7361596009975062  *\n",
    "    10: cv_score = 0.7222967261625052 f1_score = 0.7465753424657535\n",
    "    20: cv_score = 0.7454181475969770 f1_score = 0.7130712008501594\n",
    "    40: cv_score = 0.7328312987036163 f1_score = 0.7467312348668281\n",
    "\n",
    "bert + meta + keyword:\n",
    "    5 : cv_score = 0.7496093435110001 f1_score = 0.7058823529411763\n",
    "    10: cv_score = 0.7441278912831047 f1_score = 0.7675417661097851  *\n",
    "    20: cv_score = 0.7357580671447163 f1_score = 0.7607768829938418\n",
    "    40: cv_score = 0.7505499734217471 f1_score = 0.7597189695550353\n",
    "    80: cv_score = 0.7519284826539710 f1_score = 0.7247081712062258\n",
    "\n",
    "bert + meta + keyword + cosineSimRatio\n",
    "    5 : cv_score = 0.7562631511302100 f1_score = 0.7270101107366395\n",
    "    10: cv_score = 0.7396842074042981 f1_score = 0.7367901234567901\n",
    "    20: cv_score = 0.7555392927016631 f1_score = 0.7443682664054848\n",
    "    40: cv_score = 0.7606594232531072 f1_score = 0.7557003257328989  *\n",
    "    \n",
    "bert + meta + keyword + cosineSimRatio + ICA:\n",
    "    5 : cv_score = 0.6739008093714172 f1_score = 0.6790123456790124\n",
    "    10: cv_score = 0.7522109377868911 f1_score = 0.7671357240127098  *\n",
    "    20: cv_score = 0.7474259878739999 f1_score = 0.7145152651312265\n",
    "    40: cv_score = 0.7546149099255375 f1_score = 0.7488986784140969 \n",
    "    80: cv_score = 0.7523111740532332 f1_score = 0.7439803439803440\n",
    "    Final F1 = 0.7376456408196062\n",
    "    \n",
    "bert + meta + keyword:\n",
    "    Best CV params = {'max_epochs': 80, 'module__hiddenLayerCounts': [20]}\n",
    "    Discrete2OneHot_FeatureTransformer: transform(): X.shape = (2721, 222)\n",
    "    ------------------------- cv_score = 0.7497306991236068 f1_score = 0.7628341111582521 \n",
    "    Final F1 = 0.7546099290780142\n",
    "\n",
    "bert:\n",
    "    Best CV params = {'module__hiddenLayerCounts': [20]}\n",
    "    ------------------------- cv_score = 0.76457992604136 f1_score = 0.760537285780454 -------------------------\n",
    "    Final F1 = 0.7514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_FCNN ():\n",
    "    \n",
    "    global trainDF, evalDF, TRAIN_ROW_COUNT, FT, HIDDEN_LayerCounts\n",
    "    TRAIN_ROW_COUNT = 8234\n",
    "    \n",
    "    train_file = \"Data/all.tsv\"\n",
    "    rawDF = pd.read_csv (train_file, sep='\\t')\n",
    "    # rawDF = rawDF[0:30]               # Comment this out ********************************\n",
    "    trainDF = rawDF[:TRAIN_ROW_COUNT]\n",
    "    rawDF = None\n",
    "    \n",
    "    X = FT.fit_transform (trainDF, trainDF['target_relabeled'])\n",
    "    Y = trainDF['target_relabeled']\n",
    "    X = X[:TRAIN_ROW_COUNT, :]\n",
    "    Y = Y[:TRAIN_ROW_COUNT]    \n",
    "    X = X.astype (np.float32)\n",
    "    Y = Y.astype (np.int64)\n",
    "    print ('train: X.shape =', X.shape)\n",
    "    \n",
    "    lrScheduler = LRScheduler (\n",
    "        \n",
    "        CyclicLR,\n",
    "        base_lr=0.0001,\n",
    "        max_lr=0.05,\n",
    "        step_every='batch'\n",
    "    )\n",
    "    clf = NeuralNetClassifier (\n",
    "\n",
    "            module=MyModule,\n",
    "            module__inputCount=X.shape[1], \n",
    "            module__outputCount=2, \n",
    "            module__hiddenLayerCounts=HIDDEN_LayerCounts,\n",
    "            max_epochs=1000,\n",
    "            # lr=0.01,\n",
    "            iterator_train__shuffle=True,\n",
    "            callbacks=[('LRScheduler', lrScheduler), ('EarlyStopping', EarlyStopping (patience=20))]\n",
    "    )\n",
    "    clf.fit (X, Y)\n",
    "    \n",
    "    test_file = \"Data/test.tsv\"\n",
    "    testDF = pd.read_csv (test_file, sep='\\t')\n",
    "    X = FT.transform (testDF)\n",
    "    X = X.astype (np.float32)\n",
    "    print ('test: X.shape =', X.shape)\n",
    "    Y = np.round (clf.predict (X)).astype (int)\n",
    "    Y_perfect = testDF['target'].astype (int)\n",
    "    f1_acc = f1_score (Y_perfect, Y)\n",
    "    print ('\\n************** F1-Score =', f1_acc, \"*************\\n\")\n",
    "    \n",
    "    submitDF = testDF[['id', 'target']]\n",
    "    submitDF['target'] = Y\n",
    "    submitDF = submitDF.astype (int)\n",
    "    submitDF.to_csv ('submission_v'+str (VERSION) + 'FCNN.csv', index=False)\n",
    "    print (\"Done Prediction !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_FCNN ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
