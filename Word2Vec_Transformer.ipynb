{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# coding: utf-8\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "import tornado\n",
    "import tornado.httpserver\n",
    "import tornado.ioloop\n",
    "from tornado import web\n",
    "import yaml\n",
    "import requests\n",
    "import traceback\n",
    "import json\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Global vars:\n",
    "global QUESTION_BANK, WV_EMBEDDINGS\n",
    "WV_EMBEDDINGS  = None    # = google embeddings : is a 5GB data structure, hence hold only 1 instance\n",
    "QUESTION_BANK  = None    # = Question_Bank() : holds all the question bank data, hence hold only 1 instance\n",
    "\n",
    "# for text cleaning\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# for word replacements\n",
    "REPLACE = {\n",
    "    'corona'              : 'virus',\n",
    "    'coronavirus'         : 'virus',\n",
    "    'covid'               : 'virus',\n",
    "    'covid-19'            : 'virus',    \n",
    "}\n",
    "DICT_FILE = 'EnglishWords.txt'\n",
    "SPELL = SpellChecker()\n",
    "SPELL.word_frequency.load_text_file (DICT_FILE)\n",
    "\n",
    "# optional vocabulary\n",
    "EMB_VOCAB_SET = None\n",
    "with open (DICT_FILE, 'r') as f:\n",
    "    EMB_VOCAB_SET = set (f.readlines())\n",
    "EMB_VOCAB_SET = {w.strip() for w in EMB_VOCAB_SET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_word (word, embeddings=None, isStemming=True, isCorrection=True):\n",
    "    \n",
    "    word = str (word)\n",
    "    stemmer = PorterStemmer ()\n",
    "    lemmatizer = WordNetLemmatizer ()\n",
    "    word = word.lower ().strip ()\n",
    "    if embeddings:\n",
    "        if not word in embeddings:        \n",
    "            word = lemmatizer.lemmatize (word)\n",
    "        if not word in embeddings and isStemming:\n",
    "            word = stemmer.stem (word)\n",
    "        if word not in embeddings and isCorrection:\n",
    "            word = SPELL.correction (word)\n",
    "    else:\n",
    "        word = lemmatizer.lemmatize (word)\n",
    "        if isStemming:\n",
    "            word = stemmer.stem (word)\n",
    "        if isCorrection and not word in EMB_VOCAB_SET:\n",
    "            word = SPELL.correction (word)        \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepare (text, embeddings=None, isStemming=True, isCorrection=True):\n",
    "    \"\"\"\n",
    "    text: a raw string        \n",
    "    return: list of strings: modified list of words of the string\n",
    "    \"\"\"\n",
    "    \n",
    "    text = str (text)\n",
    "    text = text.lower () # lowercase text\n",
    "    text = re.sub (REPLACE_BY_SPACE_RE, \" \", text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = re.sub (BAD_SYMBOLS_RE, \"\", text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    word_tokens = word_tokenize (text)\n",
    "    # delete stopwords from text\n",
    "    text = [fix_word (w, embeddings, isStemming, isCorrection) for w in word_tokens if not w in STOPWORDS]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec_wv (sent, embeddings, dim, isStemming, isCorrection):\n",
    "    \"\"\"\n",
    "    sent       : one cleaned sentence (string)\n",
    "    embeddings : embeddings dict e.g. WV_EMBEDDINGS \n",
    "    dim        : (int) dimention of a word vector (= 300 as in the google's pretrained file)\n",
    "    \"\"\"\n",
    "    \n",
    "    sent = str (sent)\n",
    "    result = np.zeros (dim)\n",
    "    if not sent is None and len (sent)>0:\n",
    "        \n",
    "        cnt = 0     \n",
    "        words = text_prepare (sent, embeddings, isStemming, isCorrection)\n",
    "        for word in words:\n",
    "            \n",
    "            if word in embeddings:\n",
    "                result += np.array (embeddings[word])\n",
    "                cnt += 1\n",
    "        if cnt != 0:\n",
    "            result /= cnt\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pretrained Google Word2Vec Embeddings\n",
    "class WVEmbeddingTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__ (self, embFile=None, vocabSet=None, isStemming=True, isCorrection=True, isinit=True):\n",
    "        \"\"\"\n",
    "        embFile  : str, filePath\n",
    "        vocabSet : Set of words (strings). if not None then self.isinit=True else False. \n",
    "                    If self.isinit==True then .fit() will do nothing, even when params are provided.\n",
    "        init:\n",
    "            if WV_EMBEDDINGS is already init then do not re-init but self.isinit=False.\n",
    "            self.dim    : int\n",
    "            self.isinit : boolean\n",
    "        \"\"\"\n",
    "        \n",
    "        self.isinit = isinit\n",
    "        self.isStemming = isStemming\n",
    "        self.isCorrection = isCorrection\n",
    "        global WV_EMBEDDINGS\n",
    "        if WV_EMBEDDINGS is None:\n",
    "            \n",
    "            WV_EMBEDDINGS = KeyedVectors.load_word2vec_format (datapath (embFile), binary=True)            \n",
    "            if vocabSet and len (vocabSet)>0:\n",
    "                \n",
    "                vocabSet = text_prepare(' '.join(vocabSet), WV_EMBEDDINGS, isStemming, isCorrection)\n",
    "                self.isinit = True\n",
    "                temp = {w:WV_EMBEDDINGS[w] for w in vocabSet if w in WV_EMBEDDINGS}\n",
    "                WV_EMBEDDINGS = temp        \n",
    "        # fix dimension\n",
    "        dim = 0\n",
    "        if not type(WV_EMBEDDINGS) is dict:\n",
    "            for w in WV_EMBEDDINGS.vocab:\n",
    "                dim = len(WV_EMBEDDINGS[w])\n",
    "                break\n",
    "        else:\n",
    "            for w in WV_EMBEDDINGS:\n",
    "                dim = len(WV_EMBEDDINGS[w])\n",
    "                break\n",
    "        self.dim = dim\n",
    "        self.embedding = WV_EMBEDDINGS\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def fit (self, Xstr, Y=None, **fit_params):\n",
    "        \"\"\"\n",
    "        Xstr : list of raw sentences (strings)\n",
    "        WV_EMBEDDINGS should be already init, then filter it as per the vocab of the Xstr\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.isinit:\n",
    "            vocabSet = {w for s in Xstr for w in text_prepare (s, self.embedding, True, True)}\n",
    "            if not vocabSet is None and len (vocabSet)>0:\n",
    "                temp = {w:self.embedding[w] for w in vocabSet if w in self.embedding}\n",
    "                self.embedding = temp\n",
    "            self.isinit = True\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, Xstr, Y=None, **fit_params):\n",
    "        \"\"\"\n",
    "        Xstr   : list of sentences (strings)\n",
    "        return : np.array(list of corresponding sentence embeddings (avg. of word embeddings of the sent))\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding = WV_EMBEDDINGS\n",
    "        X   = [sent2vec_wv (sent, self.embedding, self.dim, self.isStemming, self.isCorrection) for sent in Xstr]\n",
    "        return np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbour embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest Neighbour Question Bank\n",
    "def getCosineNearestNeighbours (q_vec, cand_vecs, k=1, dim=300):\n",
    "    \"\"\"\n",
    "    q_vec : 1 array vec of shape 1*dim\n",
    "    cand_vecs : an array of N vecs each vec is a row\n",
    "    returns the index positions of the k closest condidate vecs ones having highest cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    q_vec       = q_vec.reshape ((1, -1))\n",
    "    cand_vecs   = cand_vecs.reshape ((-1, dim))\n",
    "    cosines     = np.array (cosine_similarity (q_vec, cand_vecs)[0])\n",
    "    merged_list = list (zip (cosines, range (len (cand_vecs))))\n",
    "    # print(merged_list)\n",
    "    sorted_list = sorted (merged_list, key=lambda x: x[0], reverse=True)\n",
    "    result = [b for a,b in sorted_list]\n",
    "    if len(result) > k:\n",
    "        result = result[:k]\n",
    "    return result\n",
    "\n",
    "\n",
    "class Question_Bank:\n",
    "    \n",
    "    def __init__ (self, questionBank_filename):\n",
    "        \n",
    "        # vectorize the data and store it in mem nearest neighbor \n",
    "        self.df = pd.read_excel (questionBank_filename)\n",
    "        self.N  = self.df.shape[0]\n",
    "        self.df.columns = ['QUESTIONS', 'ANSWERS', \"META\"]\n",
    "        Xstr = list (self.df['QUESTIONS']) + list (self.df['ANSWERS']) + list (self.df['META'])\n",
    "        embTransformer = WVEmbeddingTransformer ()\n",
    "        self.CAND_VECS = embTransformer.transform (Xstr)\n",
    "        return\n",
    "    \n",
    "    def get_nearest_QA_indices (self, qn_str):\n",
    "        \n",
    "        qn_str = str (qn_str)\n",
    "        embTransformer = WVEmbeddingTransformer ()\n",
    "        q_vec = embTransformer.transform ([qn_str])\n",
    "        nearest_indices = getCosineNearestNeighbours (q_vec, self.CAND_VECS, k=4)\n",
    "        qn_idxs = []\n",
    "        for i in nearest_indices:\n",
    "            \n",
    "            if i+1 > 2*self.N:\n",
    "                if i-2*self.N not in qn_idxs:\n",
    "                    qn_idxs.append (i-2*self.N)\n",
    "            elif i+1 > self.N:\n",
    "                if i-self.N not in qn_idxs:\n",
    "                    qn_idxs.append (i-self.N)\n",
    "            elif i>=0 and i not in qn_idxs:\n",
    "                qn_idxs.append (i)\n",
    "        return qn_idxs\n",
    "\n",
    "    def ask (self, qn_str):\n",
    "        \n",
    "        qn_str = str (qn_str)\n",
    "        qn_idxs = self.get_nearest_QA_indices (qn_str)\n",
    "        df = self.df.iloc[qn_idxs].reset_index (drop=True)\n",
    "        answer = 'NOT_FOUND'\n",
    "        suggestions = []\n",
    "        if df.shape[0]>0:\n",
    "            answer = str (df[\"ANSWERS\"][0])\n",
    "        if df.shape[0]>1:\n",
    "            suggestions = df.loc[1:, [\"QUESTIONS\", \"ANSWERS\"]].reset_index(drop=True)\n",
    "        return answer, suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Pretrained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert server start\n",
    "# !bert-serving-start -cpu -max_batch_size 16 -model_dir D:/Software/uncased_L-12_H-768_A-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert client\n",
    "# bc = BertClient ()\n",
    "# bc.encode (['First do it', 'then do it right', 'then do it better'])\n",
    "# bert_emb = bc.encode (['All payments of the indebtedness evidenced by this Note, other than regularly scheduled payments, shall be applied to such indebtedness in the order of their maturities.'])\n",
    "# bert_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_cleaner_transformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, isStemming=True, isCorrection=False, embedding=None):\n",
    "        \n",
    "        self.isStemming   = isStemming\n",
    "        self.isCorrection = isCorrection\n",
    "        self.embedding    = embedding\n",
    "        return\n",
    "    \n",
    "    def fit (self, Xstr, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform (self, Xstr, y=None, **fit_params):\n",
    "        \n",
    "        Xstr   = [' '.join (text_prepare (sent, self.embedding, self.isStemming, self.isCorrection)) for sent in Xstr]\n",
    "        return Xstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess (rawfile, newFileName):\n",
    "    \n",
    "    df = pd.read_csv (rawfile)\n",
    "    Xstr = df['text']\n",
    "    text_cleaner = Text_cleaner_transformer (isCorrection=True, embedding=WV_EMBEDDINGS)\n",
    "    Xstr = text_cleaner.transform (Xstr)\n",
    "    df['text'] = Xstr\n",
    "    pd.write_csv (newFileName)\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
