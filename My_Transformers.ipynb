{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, KernelPCA, LatentDirichletAllocation, PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import random  \n",
    "\n",
    "from random import sample \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import balanced_accuracy_score, recall_score, precision_score, confusion_matrix, make_scorer\n",
    "\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vars:\n",
    "RANDOM_STATE     = 0\n",
    "KBEST_FEATURES   = 2500\n",
    "CV_SCORERS = {\n",
    "    'precision_score':         make_scorer (precision_score),\n",
    "    'recall_score':            make_scorer (recall_score),\n",
    "    'balanced_accuracy_score': make_scorer (balanced_accuracy_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTransformer (BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit (self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        if type (X) == scipy.sparse.csr.csr_matrix:\n",
    "            return X.todense ()\n",
    "        return X\n",
    "    \n",
    "    def set_params (self, **parameters):\n",
    "        for parameter, value in parameters.items ():\n",
    "            setattr (self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def get_params (self, deep=True):\n",
    "        params = {}\n",
    "        return params    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_estimator (DenseTransformer ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityTransformer (BaseEstimator, TransformerMixin):\n",
    "      \n",
    "    def fit (self, X, y=None):\n",
    "        print ('IdentityTransformer: type(X), X.shape =', type (X), X.shape)\n",
    "        return self\n",
    "    \n",
    "    def transform (self, X, y=None):\n",
    "        print ('IdentityTransformer: type(X), X.shape =', type (X), X.shape)\n",
    "        return X\n",
    "    \n",
    "    def set_params (self, **parameters):\n",
    "        for parameter, value in parameters.items ():\n",
    "            setattr (self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def get_params (self, deep=True):\n",
    "        params = {}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Get_Train_From_TrainTest_Data (BaseEstimator, TransformerMixin):\n",
    "      \n",
    "    def fit (self, X, y):\n",
    "        print ('Get_Train_From_TrainTest_Data.fit (): type(X), X.shape =', type (X), X.shape)\n",
    "        train_len = len (y[pd.isnull(y)==False])\n",
    "        X, y = X[:train_len], y[:train_len]\n",
    "        self.train_len = train_len\n",
    "        return self\n",
    "    \n",
    "    def transform (self, X, y):\n",
    "        \n",
    "        train_len = self.train_len\n",
    "        X, y = X[:train_len], y[:train_len]\n",
    "        print ('Get_Train_From_TrainTest_Data.transform (): type(X), X.shape =', type (X), X.shape)\n",
    "        return X, y\n",
    "    \n",
    "    def set_params (self, **parameters):\n",
    "        for parameter, value in parameters.items ():\n",
    "            setattr (self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def get_params (self, deep=True):\n",
    "        params = {}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitOnce_Transformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, transformer, isFit=False):\n",
    "        \n",
    "        self.isFit = isFit\n",
    "        self.transformer = transformer\n",
    "        return\n",
    "    \n",
    "    def fit (self, X, y=None, **fit_params):\n",
    "        \n",
    "        if not self.isFit:\n",
    "            self.transformer.fit (X, y, **fit_params)\n",
    "            # self.isFit = True                           # TODO: uncomment this !!!!!!!!!!\n",
    "        return self\n",
    "\n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        X = self.transformer.transform (X, **fit_params)\n",
    "        return X\n",
    "    \n",
    "    def set_params (self, **parameters):\n",
    "        for parameter, value in parameters.items ():\n",
    "            setattr (self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def get_params (self, deep=True):\n",
    "        params = {'transformer': self.transformer, 'isFit': self.isFit}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find optimal PCA dims and the same no. of NMF features\n",
    "The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction.\n",
    "We use a GridSearchCV to set the dimensionality of the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA_NMF_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=[0.90, 0.95, 0.99], C=[0.1, 1, 10], whiten=True, isNMF=True, isSparseOut=False):\n",
    "        \n",
    "        self.pca = None\n",
    "        self.nmf = None\n",
    "        self.n_components = n_components\n",
    "        self.C   = C\n",
    "        self.whiten = whiten\n",
    "        self.isNMF = isNMF\n",
    "        self.isSparseOut = isSparseOut\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        if self.isNMF:\n",
    "            print ('Find optimal PCA dims and the same no. of NMF features for X.shape =', X.shape)\n",
    "        else:\n",
    "            print ('Find optimal PCA dims for X.shape =', X.shape)\n",
    "        self.pca = PCA (whiten=self.whiten, random_state=0)\n",
    "        classifier = LinearSVC (penalty=\"l2\", class_weight='balanced', random_state=0)\n",
    "        pipeline = Pipeline (steps=[('pca', self.pca), ('classifier', classifier)])\n",
    "        param_grid = {\n",
    "            'pca__n_components' : self.n_components,\n",
    "            'classifier__C'     : self.C\n",
    "        }\n",
    "        if type (X) == scipy.sparse.csr.csr_matrix:\n",
    "            X = X.todense ()\n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5, scoring=CV_SCORERS, \n",
    "                                     refit='balanced_accuracy_score', n_jobs=-1)\n",
    "        gridSearchCV.fit (X, Y)                   # Does not automatically sets the params of the pca or pipe\n",
    "        print (\"Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        \n",
    "        # Now use the optimal params to fit the PCA\n",
    "        self.pca = PCA (n_components=gridSearchCV.best_params_['pca__n_components'], whiten=self.whiten, random_state=0)\n",
    "        self.pca.fit (X)\n",
    "        pcaDim = self.pca.transform (X[:2,:]).shape[1]\n",
    "        # or simply use to get already fitted best estimator: self.pca = gridSearchCV.best_estimator_\n",
    "        print(\"PCA dimensionality, explainedVarRatio = \", pcaDim, self.pca.n_components)\n",
    "        # global RESULTS\n",
    "        # RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        \n",
    "        if self.isNMF :\n",
    "            self.nmf = NMF (n_components=pcaDim, random_state=1, alpha=.1, l1_ratio=.5)\n",
    "            self.nmf.fit (X)        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        if type (X) == scipy.sparse.csr.csr_matrix:\n",
    "            \n",
    "            X = X.todense()\n",
    "        X_pca = self.pca.transform (X)\n",
    "        if self.isNMF :\n",
    "            \n",
    "            X_nmf = self.nmf.transform (X)\n",
    "            if X.ndim==1 :\n",
    "\n",
    "                X = np.concatenate ([X_pca, X_nmf])\n",
    "            else:\n",
    "\n",
    "                X = np.hstack ([X_pca, X_nmf])\n",
    "        else:\n",
    "            X = X_pca\n",
    "        if self.isSparseOut and type (X) is np.ndarray:\n",
    "            \n",
    "            X = csr_matrix (X)\n",
    "        return X\n",
    "    \n",
    "    def set_params (self, **parameters):\n",
    "        for parameter, value in parameters.items ():\n",
    "            setattr (self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def get_params (self, deep=True):\n",
    "        params = {'n_components': self.n_components, 'C': self.C, 'whiten': self.whiten, \n",
    "                  'isNMF': self.isNMF, 'isSparseOut': self.isSparseOut}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA_NMF_TrainTest_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=[0.90, 0.95, 0.99], C=[0.1, 1, 10], isNMF=True, whiten=True, isSparseOut=False):\n",
    "        \n",
    "        self.pca = None\n",
    "        self.nmf = None\n",
    "        self.n_components = n_components\n",
    "        self.C   = C\n",
    "        self.whiten = whiten\n",
    "        self.isNMF = isNMF\n",
    "        self.isSparseOut = isSparseOut\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        if self.isNMF:\n",
    "            print ('Find optimal PCA dims and the same no. of NMF features for X.shape =', X.shape)\n",
    "        else:\n",
    "            print ('Find optimal PCA dims for X.shape =', X.shape)\n",
    "        if type (X) == scipy.sparse.csr.csr_matrix:\n",
    "            X = X.todense()\n",
    "        train_len = len (Y[pd.isnull(Y)==False])\n",
    "        best_params_score = []\n",
    "        for nc in self.n_components:\n",
    "            \n",
    "            pca = PCA (nc, whiten=self.whiten, random_state=0)\n",
    "            X_pca = pca.fit_transform (X)        \n",
    "            classifier = LinearSVC (penalty=\"l2\", class_weight='balanced', random_state=0)\n",
    "            param_grid = {'C' : self.C}\n",
    "            gridSearchCV = GridSearchCV (classifier, param_grid, iid=False, cv=5, scoring=CV_SCORERS, \n",
    "                                         refit='balanced_accuracy_score', n_jobs=-1)\n",
    "            gridSearchCV.fit (X_pca[:train_len], Y[:train_len])        # Does not automatically sets the params of the pca or pipe\n",
    "            best_params_score.append (((nc, gridSearchCV.best_params_['C']), gridSearchCV.best_score_))\n",
    "        best_params_score = max (best_params_score, key = lambda i : i[1])\n",
    "        print (\"--------------------- Best:  parameters =\", best_params_score[0], \", CV =\", best_params_score[1])\n",
    "        \n",
    "        # Now use the optimal params to fit the PCA\n",
    "        self.pca = PCA (n_components=best_params_score[0][0], whiten=self.whiten, random_state=0)\n",
    "        self.pca.fit (X)\n",
    "        pcaDim = self.pca.transform (X[:2,:]).shape[1]\n",
    "        # or simply use to get already fitted best estimator: self.pca = gridSearchCV.best_estimator_\n",
    "        print(\"PCA dimensionality, explainedVarRatio = \", pcaDim, self.pca.n_components)\n",
    "        # global RESULTS\n",
    "        # RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        \n",
    "        if self.isNMF :\n",
    "            self.nmf = NMF (n_components=pcaDim, random_state=1, alpha=.1, l1_ratio=.5)\n",
    "            self.nmf.fit (X)        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        if type (X) == scipy.sparse.csr.csr_matrix:\n",
    "            \n",
    "            X = X.todense()\n",
    "        X_pca = self.pca.transform (X)\n",
    "        if self.isNMF :\n",
    "            \n",
    "            X_nmf = self.nmf.transform (X)\n",
    "            if X.ndim==1 :\n",
    "\n",
    "                X = np.concatenate ([X_pca, X_nmf])\n",
    "            else:\n",
    "\n",
    "                X = np.hstack ([X_pca, X_nmf])\n",
    "        else:\n",
    "            X = X_pca\n",
    "        if self.isSparseOut and type (X) is np.ndarray:\n",
    "            \n",
    "            X = csr_matrix (X)\n",
    "        return X\n",
    "    \n",
    "    def set_params (self, **parameters):\n",
    "        for parameter, value in parameters.items ():\n",
    "            setattr (self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def get_params (self, deep=True):\n",
    "        params = {'n_components': self.n_components, 'C': self.C, 'whiten': self.whiten, \n",
    "                  'isNMF': self.isNMF, 'isSparseOut': self.isSparseOut}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA is for dense matrices. For sparse matrices use SVD below or SparsePCA in scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD_NMF_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=[0.001, 0.01, 0.1, 0.3], C=[0.1, 1, 10], isNMF=True, whiten=True, isSparseOut=False):\n",
    "        \n",
    "        self.svd = None\n",
    "        self.nmf = None\n",
    "        self.n_components = n_components\n",
    "        self.C   = C\n",
    "        self.isNMF = isNMF\n",
    "        self.whiten = whiten\n",
    "        self.isSparseOut = isSparseOut\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        if self.isNMF:\n",
    "            print ('Find optimal SVD dims and the same no. of NMF features for X.shape =', X.shape)\n",
    "        else:\n",
    "            print ('Find optimal SVD dims for X.shape =', X.shape)\n",
    "        self.svd = TruncatedSVD (whiten=self.whiten, random_state=0)\n",
    "        n_components = [int (i * X.shape[1]) for i in self.n_components if int (i * X.shape[1]) > 0]\n",
    "        # if too many components then limit upto 3000 due to memory constraints\n",
    "        if n_components[-1] > 3000:\n",
    "            n_components = [100, 800, 2000, 3000]\n",
    "        classifier = LinearSVC (penalty=\"l2\", class_weight='balanced', random_state=0)\n",
    "        pipeline   = Pipeline (steps=[('svd', self.svd), ('classifier', classifier)])\n",
    "        param_grid = {\n",
    "            'svd__n_components' : n_components,\n",
    "            'classifier__C'     : self.C\n",
    "        }\n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5, scoring=CV_SCORERS, \n",
    "                                     refit='balanced_accuracy_score', n_jobs=-1)\n",
    "        gridSearchCV.fit (X, Y)                # Does not automatically sets the params of the pca or pipe\n",
    "        print (\"Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        \n",
    "        # Now use the optimal params to fit the SVD\n",
    "        self.svd = TruncatedSVD (n_components=gridSearchCV.best_params_['svd__n_components'], whiten=self.whiten, random_state=0)\n",
    "        self.svd.fit (X)\n",
    "        svdDim = self.svd.transform (X[:2,:]).shape[1]\n",
    "        # or simply use to get already fitted best estimator: self.pca = gridSearchCV.best_estimator_\n",
    "        print(\"SVD dimensionality, explainedVarRatio = \", svdDim, self.svd.explained_variance_ratio_.sum())\n",
    "        # global RESULTS\n",
    "        # RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        \n",
    "        if self.isNMF :\n",
    "            \n",
    "            self.nmf = NMF (n_components=svdDim, random_state=1, alpha=.1, l1_ratio=.5)\n",
    "            self.nmf.fit (X)        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        X_svd = self.svd.transform (X)\n",
    "        if self.isNMF:\n",
    "            X_nmf = self.nmf.transform (X)\n",
    "            if X.ndim==1 :\n",
    "\n",
    "                X = np.concatenate ([X_svd, X_nmf])\n",
    "            else:\n",
    "\n",
    "                X = np.hstack ([X_svd, X_nmf])\n",
    "        else:\n",
    "            X = X_svd\n",
    "        if self.isSparseOut and type (X) is np.ndarray:\n",
    "            \n",
    "            X = csr_matrix (X)\n",
    "        return X\n",
    "    \n",
    "    def set_params (self, **parameters):\n",
    "        for parameter, value in parameters.items ():\n",
    "            setattr (self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def get_params (self, deep=True):\n",
    "        params = {'n_components': self.n_components, 'C': self.C, 'whiten': self.whiten, \n",
    "                  'isNMF': self.isNMF, 'isSparseOut': self.isSparseOut}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD_NMF_TrainTest_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=[0.001, 0.01, 0.1, 0.3], C=[0.1, 1, 10], isNMF=True, whiten=True, isSparseOut=False):\n",
    "        \n",
    "        self.svd = None\n",
    "        self.nmf = None\n",
    "        self.n_components = n_components\n",
    "        self.C   = C\n",
    "        self.isNMF = isNMF\n",
    "        self.whiten = whiten\n",
    "        self.isSparseOut = isSparseOut\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        if self.isNMF:\n",
    "            print ('Find optimal SVD dims and the same no. of NMF features for X.shape =', X.shape)\n",
    "        else:\n",
    "            print ('Find optimal SVD dims for X.shape =', X.shape)\n",
    "        n_components = [int (i * X.shape[1]) for i in self.n_components if int (i * X.shape[1]) > 0]\n",
    "        # if too many components then limit upto 3000 due to memory constraints\n",
    "        if n_components[-1] > 3000:\n",
    "            n_components = [100, 800, 2000, 3000]\n",
    "        train_len = len (Y[pd.isnull(Y)==False])\n",
    "        best_params_score = []\n",
    "        for nc in n_components:\n",
    "            \n",
    "            svd = TruncatedSVD (nc, whiten=self.whiten, random_state=0)\n",
    "            X_svd = svd.fit_transform (X)        \n",
    "            classifier = LinearSVC (penalty=\"l2\", class_weight='balanced', random_state=0)\n",
    "            param_grid = {'C' : self.C}\n",
    "            gridSearchCV = GridSearchCV (classifier, param_grid, iid=False, cv=5, scoring=CV_SCORERS, \n",
    "                                         refit='balanced_accuracy_score', n_jobs=-1)\n",
    "            gridSearchCV.fit (X_svd[:train_len], Y[:train_len])        # Does not automatically sets the params of the pca or pipe\n",
    "            best_params_score.append (((nc, gridSearchCV.best_params_['C']), gridSearchCV.best_score_))\n",
    "        best_params_score = max (best_params_score, key = lambda i : i[1])\n",
    "        print (\"--------------------- Best:  parameters =\", best_params_score[0], \", CV =\", best_params_score[1])\n",
    "        \n",
    "        # Now use the optimal params to fit the SVD\n",
    "        self.svd = TruncatedSVD (n_components=best_params_score[0][0], whiten=self.whiten, random_state=0)\n",
    "        self.svd.fit (X)\n",
    "        svdDim = self.svd.transform (X[:2,:]).shape[1]\n",
    "        # or simply use to get already fitted best estimator: self.pca = gridSearchCV.best_estimator_\n",
    "        print(\"SVD dimensionality, explainedVarRatio = \", svdDim, self.svd.explained_variance_ratio_.sum())\n",
    "        # global RESULTS\n",
    "        # RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        \n",
    "        if self.isNMF :\n",
    "            \n",
    "            self.nmf = NMF (n_components=svdDim, random_state=1, alpha=.1, l1_ratio=.5)\n",
    "            self.nmf.fit (X)        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        X_svd = self.svd.transform (X)\n",
    "        if self.isNMF:\n",
    "            X_nmf = self.nmf.transform (X)\n",
    "            if X.ndim==1 :\n",
    "\n",
    "                X = np.concatenate ([X_svd, X_nmf])\n",
    "            else:\n",
    "\n",
    "                X = np.hstack ([X_svd, X_nmf])\n",
    "        else:\n",
    "            X = X_svd\n",
    "        if self.isSparseOut and type (X) is np.ndarray:\n",
    "            \n",
    "            X = csr_matrix (X)\n",
    "        return X\n",
    "    \n",
    "    def set_params (self, **parameters):\n",
    "        for parameter, value in parameters.items ():\n",
    "            setattr (self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def get_params (self, deep=True):\n",
    "        params = {'n_components': self.n_components, 'C': self.C, 'whiten': self.whiten, \n",
    "                  'isNMF': self.isNMF, 'isSparseOut': self.isSparseOut}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICA_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=[0.1, 0.8], C=[0.1, 1, 10], whiten=True, isSparseOut=False):\n",
    "        \n",
    "        self.ica = None\n",
    "        self.n_components = n_components\n",
    "        self.C = C\n",
    "        self.whiten = whiten\n",
    "        self.isSparseOut = isSparseOut\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        print ('ICA_FeatureTransformer: type(X), X.shape =', type(X), X.shape)\n",
    "        self.ica = FastICA (random_state=0, whiten=self.whiten)\n",
    "        classifier = LinearSVC (penalty=\"l2\", class_weight='balanced', random_state=0)\n",
    "        pipeline = Pipeline (steps=[('ica', self.ica), ('classifier', classifier)])\n",
    "        n_components = [int (i* X.shape[1]) for i in self.n_components if int (i* X.shape[1]) > 0]\n",
    "        if not n_components:\n",
    "            n_components = [2]\n",
    "        param_grid = {\n",
    "            'ica__n_components' : n_components,\n",
    "            'classifier__C'     : self.C\n",
    "        }\n",
    "        if type (X) == scipy.sparse.csr.csr_matrix:\n",
    "            \n",
    "            X = X.todense ()\n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5, scoring=CV_SCORERS, \n",
    "                                     refit='balanced_accuracy_score', n_jobs=-1)\n",
    "        gridSearchCV.fit (X, Y)  # Does not automatically sets the params of the pca or pipe\n",
    "        print (\"Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        \n",
    "        # Now use the optimal params to fit the PCA\n",
    "        self.ica = FastICA (n_components=gridSearchCV.best_params_['ica__n_components'], whiten=self.whiten, random_state=0)\n",
    "        self.ica.fit (X)\n",
    "        icaDim = self.ica.transform (X[:2,:]).shape[1]\n",
    "        # or simply use to get already fitted best estimator: self.pca = gridSearchCV.best_estimator_\n",
    "        print (\"ICA dimensionality, explainedVarRaio = \", icaDim, self.ica.n_components)\n",
    "        # global RESULTS\n",
    "        # RESULTS.append (benchmark (gridSearchCV.best_estimator_))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        if type (X) == scipy.sparse.csr.csr_matrix:\n",
    "            X = X.todense ()\n",
    "        X = self.ica.transform (X)\n",
    "        if self.isSparseOut and type (X) is np.ndarray:\n",
    "            \n",
    "            X = csr_matrix (X)\n",
    "        return X\n",
    "    \n",
    "    def set_params (self, **parameters):\n",
    "        for parameter, value in parameters.items ():\n",
    "            setattr (self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def get_params (self, deep=True):\n",
    "        params = {'n_components': self.n_components, 'C': self.C, 'whiten': self.whiten, 'isSparseOut': self.isSparseOut}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICA_TrainTest_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=[0.8, 0.9], C=[0.1, 1, 10], whiten=True, isSparseOut=False):\n",
    "        \n",
    "        self.ica = None\n",
    "        self.n_components = n_components\n",
    "        self.C = C\n",
    "        self.whiten = whiten\n",
    "        self.isSparseOut = isSparseOut\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        print ('ICA_FeatureTransformer: type(X), X.shape =', type(X), X.shape)\n",
    "        if type (X) == scipy.sparse.csr.csr_matrix:\n",
    "            \n",
    "            X = X.todense ()\n",
    "        n_components = [int (i* X.shape[1]) for i in self.n_components if int (i* X.shape[1]) > 0]\n",
    "        if not n_components:\n",
    "            n_components = [2]\n",
    "        train_len = len (Y[pd.isnull(Y)==False])\n",
    "        best_params_score = []\n",
    "        for nc in n_components:\n",
    "            \n",
    "            ica = FastICA (nc, random_state=0, whiten=self.whiten)\n",
    "            X_ica = ica.fit_transform (X)        \n",
    "            classifier = LinearSVC (penalty=\"l2\", class_weight='balanced', random_state=0)\n",
    "            param_grid = {'C' : self.C}\n",
    "            gridSearchCV = GridSearchCV (classifier, param_grid, iid=False, cv=5, scoring=CV_SCORERS, \n",
    "                                         refit='balanced_accuracy_score', n_jobs=-1)\n",
    "            gridSearchCV.fit (X_ica[:train_len], Y[:train_len])        # Does not automatically sets the params of the pca or pipe\n",
    "            best_params_score.append (((nc, gridSearchCV.best_params_['C']), gridSearchCV.best_score_))\n",
    "        best_params_score = max (best_params_score, key = lambda i : i[1])\n",
    "        print (\"--------------------- Best:  parameters =\", best_params_score[0], \", CV =\", best_params_score[1])\n",
    "        \n",
    "        # Now use the optimal params to fit the PCA\n",
    "        self.ica = FastICA (n_components=best_params_score[0][0], whiten=self.whiten, random_state=0)\n",
    "        self.ica.fit (X)\n",
    "        icaDim = self.ica.transform (X[:2,:]).shape[1]\n",
    "        # or simply use to get already fitted best estimator: self.pca = gridSearchCV.best_estimator_\n",
    "        print (\"ICA dimensionality, explainedVarRaio = \", icaDim, self.ica.n_components)\n",
    "        # global RESULTS\n",
    "        # RESULTS.append (benchmark (gridSearchCV.best_estimator_))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        if type (X) == scipy.sparse.csr.csr_matrix:\n",
    "            X = X.todense ()\n",
    "        X = self.ica.transform (X)\n",
    "        if self.isSparseOut and type (X) is np.ndarray:\n",
    "            \n",
    "            X = csr_matrix (X)\n",
    "        return X\n",
    "    \n",
    "    def set_params (self, **parameters):\n",
    "        for parameter, value in parameters.items ():\n",
    "            setattr (self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def get_params (self, deep=True):\n",
    "        params = {'n_components': self.n_components, 'C': self.C, 'whiten': self.whiten, 'isSparseOut': self.isSparseOut}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA features\n",
    "The KPCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction. We use a GridSearchCV to set the dimensionality of the KPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPCA_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, kernel=['linear', 'poly', 'rbf', 'cosine'], C=[0.1, 1, 10], isSparseOut=False):\n",
    "        \n",
    "        self.kpca   = None\n",
    "        self.kernel = kernel \n",
    "        self.C = C\n",
    "        self.isSparseOut = isSparseOut\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        print ('Kernel PCA features for X.shape =', X.shape)\n",
    "        self.kpca   = KernelPCA (remove_zero_eig=True, random_state=0)\n",
    "        classifier = LinearSVC (penalty=\"l2\", class_weight='balanced', random_state=0)\n",
    "        pipeline = Pipeline (steps=[('kpca', self.kpca), ('classifier', classifier)])\n",
    "        param_grid = {\n",
    "            'kpca__kernel': self.kernel,\n",
    "            'classifier__C' : self.C\n",
    "        }        \n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5, scoring=CV_SCORERS, \n",
    "                                     refit='balanced_accuracy_score', n_jobs=-1)\n",
    "        gridSearchCV.fit (X, Y)       # Does not automatically sets the params of the pca\n",
    "        print (\"Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        \n",
    "        # Now use the optimal params to fit the KPCA\n",
    "        self.kpca = KernelPCA (kernel=gridSearchCV.best_params_['kpca__kernel'], remove_zero_eig=True, random_state=0)\n",
    "        self.kpca.fit (X)\n",
    "        kpcaDim = self.kpca.transform (X[:2,:]).shape[1]\n",
    "        print ('KPCA dim =', kpcaDim)\n",
    "        # or simply use to get already fitted best estimator: self.kpca = gridSearchCV.best_estimator_\n",
    "        # global RESULTS\n",
    "        # RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \"\"\"\n",
    "        X = sparse matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        X = self.kpca.transform (X)        \n",
    "        if self.isSparseOut and type (X) is np.ndarray:\n",
    "            \n",
    "            X = csr_matrix (X)\n",
    "        return X\n",
    "    \n",
    "    def set_params (self, **parameters):\n",
    "        for parameter, value in parameters.items ():\n",
    "            setattr (self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def get_params (self, deep=True):\n",
    "        params = {'kernel': self.kernel, 'C': self.C, 'isSparseOut': self.isSparseOut}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPCA_TrainTest_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, kernel=['linear', 'poly', 'rbf', 'cosine'], C=[0.1, 1, 10], isSparseOut=False):\n",
    "        \n",
    "        self.kpca   = None\n",
    "        self.kernel = kernel \n",
    "        self.C = C\n",
    "        self.isSparseOut = isSparseOut\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        print ('Kernel PCA features for X.shape =', X.shape)\n",
    "        train_len = len (Y[pd.isnull(Y)==False])\n",
    "        best_params_score = []\n",
    "        for k in self.kernel:\n",
    "            \n",
    "            kpca = KernelPCA (kernel=k, random_state=0)\n",
    "            X_kpca = kpca.fit_transform (X)        \n",
    "            classifier = LinearSVC (penalty=\"l2\", class_weight='balanced', random_state=0)\n",
    "            param_grid = {'C' : self.C}\n",
    "            gridSearchCV = GridSearchCV (classifier, param_grid, iid=False, cv=5, scoring=CV_SCORERS, \n",
    "                                         refit='balanced_accuracy_score', n_jobs=-1)\n",
    "            gridSearchCV.fit (X_kpca[:train_len], Y[:train_len])        # Does not automatically sets the params of the pca or pipe\n",
    "            best_params_score.append (((k, gridSearchCV.best_params_['C']), gridSearchCV.best_score_))\n",
    "        best_params_score = max (best_params_score, key = lambda i : i[1])\n",
    "        print (\"--------------------- Best:  parameters =\", best_params_score[0], \", CV =\", best_params_score[1])\n",
    "        \n",
    "        # Now use the optimal params to fit the KPCA\n",
    "        self.kpca = KernelPCA (kernel=best_params_score[0][0], random_state=0)\n",
    "        self.kpca.fit (X)\n",
    "        kpcaDim = self.kpca.transform (X[:2,:]).shape[1]\n",
    "        print ('KPCA dim =', kpcaDim)\n",
    "        # or simply use to get already fitted best estimator: self.kpca = gridSearchCV.best_estimator_\n",
    "        # global RESULTS\n",
    "        # RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \"\"\"\n",
    "        X = sparse matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        X = self.kpca.transform (X)        \n",
    "        if self.isSparseOut and type (X) is np.ndarray:\n",
    "            \n",
    "            X = csr_matrix (X)\n",
    "        return X\n",
    "    \n",
    "    def set_params (self, **parameters):\n",
    "        for parameter, value in parameters.items ():\n",
    "            setattr (self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def get_params (self, deep=True):\n",
    "        params = {'kernel': self.kernel, 'C': self.C, 'isSparseOut': self.isSparseOut}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation Features\n",
    "The LDA does an unsupervised dimensionality reduction, while the logistic regression does the prediction. We use a GridSearchCV to set the dimensionality of the LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=[0.05, 0.1], C=[0.1, 1, 10]):\n",
    "        \n",
    "        self.lda = None\n",
    "        self.C = C\n",
    "        self.n_components = n_components\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        print ('LDA Features for X.shape =', X.shape)\n",
    "        self.lda          = LatentDirichletAllocation (max_iter=6, learning_offset=50.0, random_state=0)\n",
    "        # convert fractions to int feature count\n",
    "        n_components = ((np.array(self.n_components) * min (X.shape[0],X.shape[1])).astype (int)).tolist ()\n",
    "        n_components = [nc for nc in n_components if nc>0]\n",
    "        if not n_components:\n",
    "            n_components = [2]\n",
    "        classifier = LinearSVC (penalty=\"l2\", class_weight='balanced', random_state=0)\n",
    "        pipeline = Pipeline (steps=[('LDA', self.lda), ('classifier', classifier)])\n",
    "        param_grid = {'LDA__n_components' : n_components, 'classifier__C': self.C}\n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5, scoring=CV_SCORERS, \n",
    "                                     refit='balanced_accuracy_score', n_jobs=-1)\n",
    "        gridSearchCV.fit (X, Y)              # Does not automatically sets the params of the pca\n",
    "        print (\"Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        \n",
    "        # Now use the optimal params to fit the LDA\n",
    "        n_components = gridSearchCV.best_params_['LDA__n_components']\n",
    "        self.lda     = LatentDirichletAllocation (n_components=n_components, max_iter=5, learning_offset=50., random_state=0)\n",
    "        self.lda.fit (X) \n",
    "        # or simply use to get already fitted best estimator: self.lda = gridSearchCV.best_estimator_\n",
    "        # global RESULTS\n",
    "        # RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        return self.lda.transform (X)\n",
    "    \n",
    "    def set_params (self, **parameters):\n",
    "        for parameter, value in parameters.items ():\n",
    "            setattr (self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def get_params (self, deep=True):\n",
    "        params = {'n_components': self.n_components, 'C': self.C}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA_TrainTest_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=[0.05, 0.2], C=[0.1, 1, 10]):\n",
    "        \n",
    "        self.lda          = None\n",
    "        self.n_components = n_components\n",
    "        self.C = C\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        print ('LDA Features for X.shape =', X.shape)\n",
    "        # convert fractions to int feature count\n",
    "        n_components = ((np.array(self.n_components) * min (X.shape[0],X.shape[1])).astype (int)).tolist ()\n",
    "        n_components = [nc for nc in n_components if nc>0]\n",
    "        if not n_components:\n",
    "            n_components = [2]\n",
    "        train_len = len (Y[pd.isnull(Y)==False])\n",
    "        best_params_score = []\n",
    "        for nc in n_components:\n",
    "            \n",
    "            lda = LatentDirichletAllocation (max_iter=6, learning_offset=50.0, random_state=0)\n",
    "            X_lda = lda.fit_transform (X)        \n",
    "            classifier = LinearSVC (penalty=\"l2\", class_weight='balanced', random_state=0)\n",
    "            param_grid = {'C' : self.C}\n",
    "            gridSearchCV = GridSearchCV (classifier, param_grid, iid=False, cv=5, scoring=CV_SCORERS, \n",
    "                                         refit='balanced_accuracy_score', n_jobs=-1)\n",
    "            gridSearchCV.fit (X_lda[:train_len], Y[:train_len])        # Does not automatically sets the params of the pca or pipe\n",
    "            best_params_score.append (((nc, gridSearchCV.best_params_['C']), gridSearchCV.best_score_))\n",
    "        best_params_score = max (best_params_score, key = lambda i : i[1])\n",
    "        print (\"--------------------- Best:  parameters =\", best_params_score[0], \", CV =\", best_params_score[1])\n",
    "        \n",
    "        # Now use the optimal params to fit the LDA\n",
    "        n_components = best_params_score[0][0]\n",
    "        self.lda     = LatentDirichletAllocation (n_components=n_components, max_iter=5, learning_offset=50., random_state=0)\n",
    "        self.lda.fit (X) \n",
    "        # or simply use to get already fitted best estimator: self.lda = gridSearchCV.best_estimator_\n",
    "        # global RESULTS\n",
    "        # RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        return self.lda.transform (X)\n",
    "    \n",
    "    def set_params (self, **parameters):\n",
    "        for parameter, value in parameters.items ():\n",
    "            setattr (self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def get_params (self, deep=True):\n",
    "        params = {'n_components': self.n_components, 'C': self.C}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find K-best features based on Mutual-Info / F-Score / Chi^2\n",
    "Use K-best feature selection with logistic regression classifier in GridSearchCV to find optimal val of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectKBest_feature_selector (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=[0.75, 0.80, 0.90, 0.95, 1.0], score_func=[chi2, mutual_info_classif], C=[0.001, 0.005, 0.01, 0.1, 1, 10]):\n",
    "        \n",
    "        self.score_func   = score_func\n",
    "        self.n_components = n_components\n",
    "        self.selectKBest  = None\n",
    "        self.C = C\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        train_len = len (Y[pd.isnull(Y)==False])\n",
    "        X, Y = X[:train_len], Y[:train_len]\n",
    "        print ('Find K-best features based on Mutual-Info / Chi^2 for X.shape =', X.shape)\n",
    "        self.selectKBest  = SelectKBest ()\n",
    "        # this works only when X >= 0, hence shift by a constant so that it is >=0\n",
    "        if X.min () < 0:\n",
    "            \n",
    "            shift_k = np.abs (X.min ())\n",
    "            X[X.nonzero ()] = X[X.nonzero ()] + shift_k\n",
    "        # convert fractions to int feature count\n",
    "        n_components = ((np.array(self.n_components) * X.shape[1]).astype (int)).tolist ()\n",
    "        n_components = [nc for nc in n_components if nc>0]\n",
    "        if not n_components:\n",
    "            n_components = [2]\n",
    "        lsvc = LinearSVC (penalty=\"l2\", class_weight='balanced', random_state=0)  \n",
    "        pipeline = Pipeline (steps=[('selectKBest', self.selectKBest), ('classifier', lsvc)])\n",
    "        param_grid = {            \n",
    "            'selectKBest__k'          : n_components,\n",
    "            'selectKBest__score_func' : self.score_func,\n",
    "            'classifier__C'           : self.C\n",
    "        }\n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5, scoring=CV_SCORERS, \n",
    "                                     refit='balanced_accuracy_score', n_jobs=-1)\n",
    "        gridSearchCV.fit (X, Y)  # Does not automatically sets the params of the models oe pipeline\n",
    "        print (\"--------------------- Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        \n",
    "        # Now use the optimal params to fit the model\n",
    "        k          = gridSearchCV.best_params_['selectKBest__k']\n",
    "        score_func = gridSearchCV.best_params_['selectKBest__score_func']\n",
    "        # print(gridSearchCV.grid_scores_)\n",
    "        # Now create the model with the best params and fit over the data\n",
    "        self.selectKBest = SelectKBest (score_func, k)\n",
    "        self.selectKBest.fit (X, Y)\n",
    "        # or simply use to get already fitted best estimator: self.selectKBest = gridSearchCV.best_estimator_\n",
    "        # global RESULTS\n",
    "        # RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        return self\n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        return self.selectKBest.transform (X)\n",
    "    \n",
    "    def set_params (self, **parameters):\n",
    "        for parameter, value in parameters.items ():\n",
    "            setattr (self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def get_params (self, deep=True):\n",
    "        params = {'n_components': self.n_components, 'score_func': self.score_func, 'C': self.C}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Through L1-regularized SVM similar to Lasso, identify the useful sparse features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseSVM_feature_selector (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, tol=[1e-3, 1e-4, 1e-5, 1e-6], C=[0.001, 0.005, 0.01, 0.1, 1, 10]): \n",
    "        \n",
    "        self.tol = tol\n",
    "        self.C   = C\n",
    "        self.sfm = None\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        train_len = len (Y[pd.isnull(Y)==False])\n",
    "        X, Y = X[:train_len], Y[:train_len]\n",
    "        print ('LinearSVC with L1-based feature selection for X.shape =', X.shape)\n",
    "        # The smaller C, the stronger the regularization.\n",
    "        # The more regularization, the more sparsity.\n",
    "        pipeline = Pipeline ([\n",
    "          ('feature_selection', SelectFromModel (LinearSVC (penalty=\"l1\", dual=False, class_weight='balanced', random_state=0))),\n",
    "          ('classifier', LinearSVC (penalty=\"l2\", class_weight='balanced', random_state=0))  ])\n",
    "        param_grid = {'feature_selection__estimator__tol' : self.tol, 'classifier__C': self.C}\n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5, scoring=CV_SCORERS, \n",
    "                                     refit='balanced_accuracy_score', n_jobs=-1)\n",
    "        gridSearchCV.fit (X, Y)  # Does not automatically sets the params of the models\n",
    "        print (\"--------------------- Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        \n",
    "        # Now use the optimal params to fit the model\n",
    "        tol = gridSearchCV.best_params_['feature_selection__estimator__tol']\n",
    "        svc = LinearSVC (penalty=\"l1\", dual=False, tol=tol, random_state=0)\n",
    "        # svc.fit (X,Y)\n",
    "        self.sfm = SelectFromModel (svc)\n",
    "        self.sfm.fit (X, Y)\n",
    "        # or simply use to get already fitted best estimator: self.sfm = gridSearchCV.best_estimator_\n",
    "        print ('New #features = ', self.sfm.transform (X[:2,:]).shape[1] )\n",
    "        # global RESULTS\n",
    "        # RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        return self\n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        return self.sfm.transform (X)\n",
    "    \n",
    "    def set_params (self, **parameters):\n",
    "        for parameter, value in parameters.items ():\n",
    "            setattr (self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def get_params (self, deep=True):\n",
    "        params = {'tol': self.tol}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection based on clf.feature_importances_ with CV on threshold\n",
    "clf = RandomForest / Xgboost / CatBoost\n",
    "we use CV to determine threshold for SelectFromModel( rf_clf, threshold=? )\n",
    "\n",
    "@param: clf: an already fitted(X,Y) clf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLF_importance_feature_selector (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, threshold=[1e-9, 1e-8, 1e-7]):\n",
    "        \n",
    "        self.clf             = None\n",
    "        self.selectFromModel = None\n",
    "        self.threshold       = threshold\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):       \n",
    "        \n",
    "        train_len = len (Y[pd.isnull(Y)==False])\n",
    "        X, Y = X[:train_len], Y[:train_len]\n",
    "        print ('Feature Selection based on rf.feature_importances_ for X.shape =', X.shape)\n",
    "        self.clf = RandomForestClassifier ()\n",
    "        pipeline = Pipeline ([\n",
    "          ('feature_selection', SelectFromModel (self.clf)),\n",
    "          ('classification', LinearSVC (penalty=\"l2\", class_weight='balanced', random_state=0))  ])\n",
    "        param_grid = {'feature_selection__threshold' : self.threshold}        \n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5, scoring=CV_SCORERS, \n",
    "                                     refit='balanced_accuracy_score', n_jobs=-1)\n",
    "        gridSearchCV.fit (X, Y)  \n",
    "        print (\"--------------------- Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        \n",
    "        self.selectFromModel = SelectFromModel (self.clf, gridSearchCV.best_params_['feature_selection__threshold'])\n",
    "        self.selectFromModel.fit (X, Y)\n",
    "        clfDim = self.selectFromModel.transform (X[:2,:]).shape[1]        \n",
    "        print ('clfDim = ', clfDim)                \n",
    "        # global RESULTS\n",
    "        # RESULTS.append (benchmark (gridSearchCV.best_estimator_))\n",
    "        return self\n",
    "        \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        return self.selectFromModel.transform (X)\n",
    "    \n",
    "    def set_params (self, **parameters):\n",
    "        for parameter, value in parameters.items ():\n",
    "            setattr (self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def get_params (self, deep=True):\n",
    "        params = {'threshold': self.threshold}\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada-Boosted Classifiers from a base clf after CV over boosting params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ab_params     = { 'n_estimators'  :  50 }\n",
    "# ab_param_grid = { 'learning_rate' : [0.5, 0.75, 1.0] }\n",
    "\n",
    "def get_clf_adaBoosted_cv (X, Y, clf, params=None, param_grid=None):\n",
    "    \n",
    "    ab_clf = AdaBoostClassifier(base_estimator=clf)\n",
    "    if params:\n",
    "        ab_clf.set_params(**params)\n",
    "    \n",
    "    if param_grid:\n",
    "        gridSearchCV = GridSearchCV (ab_clf, param_grid, iid=False, cv=5)\n",
    "        gridSearchCV.fit (X, Y)  \n",
    "        print (\"get_clf_adaBoosted_cv(): Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        ab_clf = gridSearchCV.best_estimator_\n",
    "        global RESULTS\n",
    "        RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        # TODO: Plot scores for each split, and get its' variance\n",
    "    \n",
    "    return ab_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a fully connected FCNN clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule (nn.Module):\n",
    "    \n",
    "    def __init__(self, inputCount=1024, outputCount=2, hiddenLayerCounts=[128], drop_prob=0.2, nonlin=nn.ReLU ()):\n",
    "        \n",
    "        super(MyModule, self).__init__()\n",
    "\n",
    "        self.nonlin  = nonlin\n",
    "        self.dropout = nn.Dropout (drop_prob)\n",
    "        \n",
    "        self.dense1     = nn.Linear (inputCount, hiddenLayerCounts[0])\n",
    "        self.batchnorm1 = nn.BatchNorm1d (hiddenLayerCounts[0])\n",
    "        # self.dense2     = nn.Linear(hiddenLayerCounts[0], hiddenLayerCounts[1])\n",
    "        # self.batchnorm2 = nn.BatchNorm1d (hiddenLayerCounts[1])\n",
    "        # self.dense3     = nn.Linear(hiddenLayerCounts[1], hiddenLayerCounts[2])\n",
    "        # self.batchnorm3 = nn.BatchNorm1d (hiddenLayerCounts[2])        \n",
    "        self.outDense   = nn.Linear (hiddenLayerCounts[-1], outputCount)\n",
    "        \n",
    "        self.outActivtn = None\n",
    "        if outputCount == 1 or outputCount == 2:\n",
    "            self.outActivtn = nn.Sigmoid ()\n",
    "        else:\n",
    "            self.outActivtn = nn.Softmax (dim=-1)\n",
    "        return\n",
    "\n",
    "    def forward (self, X, **kwargs):\n",
    "        \n",
    "        X = self.dropout (self.nonlin (self.batchnorm1 (self.dense1 (X))))\n",
    "        # X = self.dropout (self.nonlin (self.batchnorm2 (self.dense2 (X))))\n",
    "        # X = self.dropout (self.nonlin (batchnorm3 (self.dense3 (X))))\n",
    "        X = self.outActivtn (self.outDense (X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get optimal Classifier after CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Best_clf_cv_transformer (BaseEstimator, TransformerMixin): \n",
    "            \n",
    "    def __init__(self, myparams={'C':1}, **other_params):\n",
    "        \n",
    "        self.cv    = 5\n",
    "        if 'cv' in myparams:\n",
    "            self.cv= myparams['cv']\n",
    "        clf        =  None\n",
    "        name       = myparams['name']\n",
    "        if name   == 'Logit':\n",
    "            clf    =  LogisticRegression (random_state=0)\n",
    "        elif name == 'DT':\n",
    "            clf    =  DecisionTreeClassifier (random_state=0)\n",
    "        elif name == 'RidgClf':\n",
    "            clf    =  RidgeClassifier (random_state=0)\n",
    "        elif name == 'Prcpt':\n",
    "            clf    =  Perceptron (random_state=0)\n",
    "        elif name == 'PssAggClf':\n",
    "            clf    =  PassiveAggressiveClassifier (random_state=0)\n",
    "        elif name == 'Knn':\n",
    "            clf    =  KNeighborsClassifier (random_state=0)\n",
    "        elif name == 'RF':\n",
    "            clf    =  RandomForestClassifier (random_state=0)\n",
    "        elif name == 'NearCent':\n",
    "            clf    =  NearestCentroid (random_state=0)\n",
    "        elif name == 'MultNB':\n",
    "            clf    =  MultinomialNB (random_state=0)\n",
    "        elif name == 'BernNB':\n",
    "            clf    =  BernoulliNB (random_state=0)    \n",
    "        elif name == 'Svc':\n",
    "            clf    =  SVC (probability=True, random_state=0)\n",
    "        elif name == 'LSvc':\n",
    "            clf    =  LinearSVC (random_state=0)\n",
    "        elif name == 'Xgb':\n",
    "            clf    =  xgb.XGBClassifier (random_state=0) # XGBRFClassifier()\n",
    "        elif name == 'Catb' :                            # issues with CV\n",
    "            clf    =  CatBoostClassifier (verbose=False, random_state=0)\n",
    "        elif name == 'FCNN':\n",
    "            clf    =  None                               # a fully conn Neural net clf will be created at time of fitting\n",
    "        else:\n",
    "            print('ERROR Best_clf_cv_transformer: invalid @param name \\n')\n",
    "        self.isCV = True\n",
    "        if 'isCV' in myparams:\n",
    "            \n",
    "            self.isCV = myparams['isCV']\n",
    "        if 'params' in myparams:\n",
    "            \n",
    "            clf.set_params (**myparams['params'])\n",
    "        if other_params:\n",
    "            \n",
    "            clf.set_params (**other_params)\n",
    "        self.param_grid = None\n",
    "        if 'param_grid' in myparams:\n",
    "            \n",
    "            self.param_grid = myparams['param_grid']\n",
    "        self.myparams = myparams\n",
    "        self.clf = clf\n",
    "        self.cv_score = 0\n",
    "        self.name = name\n",
    "        self._estimator_type='classifier'\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def fit (self, X, Y):\n",
    "                \n",
    "        train_len = len (Y[pd.isnull(Y)==False])\n",
    "        X, Y = X[:train_len], Y[:train_len]\n",
    "        print ('training', self.name, 'for X.shape =', X.shape)\n",
    "        n_jobs = -1\n",
    "        if self.name == 'FCNN':\n",
    "            self.clf = NeuralNetClassifier (\n",
    "                            module=MyModule,\n",
    "                            module__inputCount=X.shape[1], \n",
    "                            module__outputCount=2, \n",
    "                            module__hiddenLayerCounts=[int (X.shape[1]/1)],\n",
    "                            max_epochs=20,\n",
    "                            lr=0.05,\n",
    "                            # Shuffle training data on each epoch\n",
    "                            iterator_train__shuffle=True,\n",
    "            )\n",
    "            X = X.astype (np.float32)\n",
    "            Y = Y.astype (np.int64)\n",
    "            n_jobs = None\n",
    "            \n",
    "        if self.isCV:\n",
    "            if self.param_grid:\n",
    "\n",
    "                gridSearchCV = GridSearchCV (\n",
    "                    self.clf, self.param_grid, iid=False, cv=self.cv, scoring=CV_SCORERS, \n",
    "                    refit='balanced_accuracy_score', n_jobs=n_jobs\n",
    "                )\n",
    "                gridSearchCV.fit (X, Y)  \n",
    "                print (self.name, \": Best_clf_cv_transformer: Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "                print (gridSearchCV.best_params_)\n",
    "                self.clf = gridSearchCV.best_estimator_\n",
    "                self.cv_score = gridSearchCV.best_score_\n",
    "                if self.name == 'LSvc':\n",
    "                    \n",
    "                    self.clf = CalibratedClassifierCV (self.clf)\n",
    "                    self.clf.fit (X, Y)\n",
    "                # global RESULTS\n",
    "                # RESULTS.append(benchmark(self.clf))\n",
    "                # TODO: Plot scores for each split, and get its' variance\n",
    "            else:\n",
    "\n",
    "                if self.name == 'LSvc':\n",
    "                    \n",
    "                    self.clf = CalibratedClassifierCV (self.clf)\n",
    "                print (self.name, ': Best_clf_cv_transformer: starting CV =', self.cv)\n",
    "                if self.name not in {'RF', 'Catb', 'FCNN'}:\n",
    "                    \n",
    "                    self.cv_score = cross_validate (self.clf, X, Y, self.cv)\n",
    "                else:\n",
    "                    \n",
    "                    voting_clf = VotingClassifier (estimators=[(self.name, self.clf)])\n",
    "                    cv_results = cross_validate (voting_clf, X, Y, cv=self.cv)\n",
    "                    self.cv_score = np.mean (cv_results['test_score'])\n",
    "                self.clf.fit (X, Y)\n",
    "                print (self.name, \": cv_score:   %0.3f\" % self.cv_score)\n",
    "        else:\n",
    "            if self.name == 'LSvc':\n",
    "                self.clf = CalibratedClassifierCV (self.clf)\n",
    "            self.clf.fit (X, Y)\n",
    "        print (\"Done Fitting\", self.name)\n",
    "        return self\n",
    "    \n",
    "    def get_cv_score (self):\n",
    "        return self.cv_score\n",
    "    \n",
    "    def transform (self, X, Y=None, **fit_params):\n",
    "        \n",
    "        if self.name == 'FCNN':\n",
    "            X = X.astype (np.float32)\n",
    "            if not Y is None:\n",
    "                Y = Y.astype (np.int64)        \n",
    "        return self.clf.transform(X, Y)\n",
    "    \n",
    "    def predict (self, X, **fit_params):\n",
    "        \n",
    "        if self.name == 'FCNN':\n",
    "            X = X.astype (np.float32)      \n",
    "        return self.clf.predict(X)\n",
    "    \n",
    "    def predict_proba (self, X):\n",
    "        \n",
    "        if self.name == 'FCNN':\n",
    "            X = X.astype (np.float32)      \n",
    "        return self.clf.predict_proba (X)\n",
    "    \n",
    "    def predict_log_proba (self, X):\n",
    "        \n",
    "        if self.name == 'FCNN':\n",
    "            X = X.astype (np.float32)\n",
    "        return self.clf.predict_log_proba (X)\n",
    "    \n",
    "    def score (self, X, Y, **fit_params):\n",
    "        \n",
    "        if self.name == 'FCNN':\n",
    "            X = X.astype (np.float32)\n",
    "        return self.clf.score (X, Y, **fit_params)\n",
    "    \n",
    "    def decision_function (self, X, **fit_params):\n",
    "        \n",
    "        if self.name == 'FCNN':\n",
    "            X = X.astype (np.float32)\n",
    "        return self.clf.decision_function (X)\n",
    "    \n",
    "    def set_params (self, **params):\n",
    "        \n",
    "        myparams = params['myparams']\n",
    "        self.cv    = 5\n",
    "        if 'cv' in myparams:\n",
    "            self.cv= myparams['cv']\n",
    "        clf        =  None\n",
    "        name       = myparams['name']\n",
    "        if name   == 'Logit':\n",
    "            clf    =  LogisticRegression (random_state=0)\n",
    "        elif name == 'DT':\n",
    "            clf    =  DecisionTreeClassifier (random_state=0)\n",
    "        elif name == 'RidgClf':\n",
    "            clf    =  RidgeClassifier (random_state=0)\n",
    "        elif name == 'Prcpt':\n",
    "            clf    =  Perceptron (random_state=0)\n",
    "        elif name == 'PssAggClf':\n",
    "            clf    =  PassiveAggressiveClassifier (random_state=0)\n",
    "        elif name == 'Knn':\n",
    "            clf    =  KNeighborsClassifier (random_state=0)\n",
    "        elif name == 'RF':\n",
    "            clf    =  RandomForestClassifier (random_state=0)\n",
    "        elif name == 'NearCent':\n",
    "            clf    =  NearestCentroid (random_state=0)\n",
    "        elif name == 'MultNB':\n",
    "            clf    =  MultinomialNB (random_state=0)\n",
    "        elif name == 'BernNB':\n",
    "            clf    =  BernoulliNB (random_state=0)    \n",
    "        elif name == 'Svc':\n",
    "            clf    =  SVC (probability=True, random_state=0)\n",
    "        elif name == 'LSvc':\n",
    "            clf    =  LinearSVC (random_state=0)\n",
    "        elif name == 'Xgb':\n",
    "            clf    =  xgb.XGBClassifier (random_state=0) # XGBRFClassifier()\n",
    "        elif name == 'Catb':                             # issues with CV\n",
    "            clf    =  CatBoostClassifier (verbose=False, random_state=0)\n",
    "        elif name != 'FCNN':\n",
    "            print('ERROR Best_clf_cv_transformer: invalid @param name \\n')\n",
    "        self.isCV = True\n",
    "        if 'isCV' in myparams:\n",
    "            \n",
    "            self.isCV = myparams['isCV']\n",
    "        if 'params' in myparams:\n",
    "            \n",
    "            clf.set_params (**myparams['params'])\n",
    "        self.param_grid = None\n",
    "        if 'param_grid' in myparams:\n",
    "            \n",
    "            self.param_grid = myparams['param_grid']\n",
    "        self.myparams = myparams\n",
    "        self.clf = clf\n",
    "        self.cv_score = 0\n",
    "        self.name = name\n",
    "        self._estimator_type='classifier'\n",
    "        return self\n",
    "    \n",
    "    def get_params (self, deep=True):\n",
    "        # params = self.clf.get_params(deep)\n",
    "        # params['myparams'] = self.myparams\n",
    "        params = {'myparams': self.myparams}\n",
    "        return params\n",
    "    \n",
    "    def apply (self, X):\n",
    "        return self.clf.apply(X)\n",
    "    \n",
    "    def decision_path (self, X):\n",
    "        return self.clf.decision_path (X)\n",
    "    \n",
    "    def staged_decision_function (self, X):\n",
    "        return self.clf.staged_decision_function (X)\n",
    "    \n",
    "    def staged_predict (self, X):\n",
    "        return self.clf.staged_predict (X)\n",
    "    \n",
    "    def staged_predict_proba (self, X):\n",
    "        return self.clf.staged_predict_proba (X)\n",
    "    \n",
    "    def staged_score (self, X):\n",
    "        return self.clf.staged_score (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bagging Classifier from a base clf, after CV on boosting params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params     = { 'max_samples'  : 1.0,  'n_estimators' : 10 }\n",
    "# param_grid = { 'max_features' : [0.7, 0.8, 0.9, 1.0] }\n",
    "\n",
    "def get_bagging_clf_cv (X, Y, clf, params=None, param_grid=None): \n",
    "    \n",
    "    bag_clf = BaggingClassifier(base_estimator=clf)\n",
    "    if params:\n",
    "        bag_clf.set_params(**params)\n",
    "    \n",
    "    if param_grid:\n",
    "        gridSearchCV = GridSearchCV (bag_clf, param_grid, iid=False, cv=5)\n",
    "        gridSearchCV.fit (X, Y)  \n",
    "        print (\"get_bagging_clf_cv(): Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        bag_clf = gridSearchCV.best_estimator_\n",
    "        global RESULTS\n",
    "        RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        # TODO: Plot scores for each split, and get its' variance\n",
    "    \n",
    "    return bag_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def getBaggedXGB_RESULTS(Xtrain, Xtest, Ytrain):\n",
    "    \n",
    "    param = {}\n",
    "    param['booster'] = 'gbtree'\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['num_class'] = 9\n",
    "    param['eval_metric'] = 'logloss'\n",
    "    param['scale_pos_weight'] = 1.0\n",
    "    param['bst:eta'] = 0.3\n",
    "    param['bst:max_depth'] = 6\n",
    "    param['bst:colsample_bytree'] = 0.5\n",
    "    param['silent'] = 1\n",
    "    param['nthread'] = 16\n",
    "    num_round = 100\n",
    "    plst = list(param.items())\n",
    "    watchlist = []\n",
    "    \n",
    "    time0 = time()\n",
    "    idxTrain = range(len(Ytrain))\n",
    "    Ytestxg  = np.zeros((Xtest.shape[0], 9))\n",
    "    \n",
    "    bgs = 2 # 20\n",
    "    for bg in range(bgs):\n",
    "        param['seed'] = bg + 1\n",
    "        plst = list(param.items())\n",
    "        \n",
    "        newidxTrain = random.sample(idxTrain, int(len(idxTrain) * 1.0))\n",
    "        \n",
    "        for i in range(int(len(idxTrain) * 7.0)):\n",
    "            newidxTrain.append(random.choice(idxTrain))\n",
    "        \n",
    "        Xdatatrain = xgb.DMatrix(data = Xtrain[newidxTrain], label = Ytrain[newidxTrain])\n",
    "        Xdatatest = xgb.DMatrix(data = Xtest)\n",
    "        \n",
    "        bst = xgb.train(plst, Xdatatrain, num_round, watchlist)\n",
    "        \n",
    "        curpred = bst.predict(Xdatatest).reshape((Xtest.shape[0], 9))        \n",
    "        Ytestxg += curpred\n",
    "        \n",
    "        print (bg, (time() - time0) / 60.)\n",
    "        \n",
    "    Ytestxg /= bgs\n",
    "    return Ytestxg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ytestxg = getBaggedXGB_RESULTS(X_TRAIN, X_TEST, Y_TRAIN)\n",
    "Ytestxg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(1000, 20, n_informative=10, random_state=0)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "\"\"\"\n",
    "clf = NeuralNetClassifier (\n",
    "                module=MyModule,\n",
    "                module__inputCount=X.shape[1], \n",
    "                module__outputCount=2, \n",
    "                module__hiddenLayerCounts=[int (X.shape[1]/4), int (X.shape[1]/8)],\n",
    "                max_epochs=20,\n",
    "                lr=0.1,\n",
    "                # Shuffle training data on each epoch\n",
    "                iterator_train__shuffle=True,\n",
    ") \"\"\"\n",
    "\n",
    "clf = Best_clf_cv_transformer ({ 'name': 'FCNN'})\n",
    "clf.fit (X, y)\n",
    "y_proba = clf.predict_proba (X)\n",
    "print (y_proba)\n",
    "# cv_score = cross_validate (clf, X, y, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very useful grid search CV strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_wrapper(X_train, y_train, refit_score='precision_score'):\n",
    "    \"\"\"\n",
    "    fits a GridSearchCV classifier using refit_score for optimization\n",
    "    prints classifier performance metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    skf = StratifiedKFold (n_splits=10)\n",
    "    grid_search = GridSearchCV (clf, param_grid, scoring=scorers, refit=refit_score,\n",
    "                                cv=skf, return_train_score=True, n_jobs=-1)\n",
    "    grid_search.fit (X_train.values, y_train.values)\n",
    "\n",
    "    # make the predictions\n",
    "    y_pred = grid_search.predict (X_test.values)\n",
    "\n",
    "    print('Best params for {}'.format (refit_score))\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # confusion matrix on the test data.\n",
    "    print ('\\nConfusion matrix of Random Forest optimized for {} on the test data:'.format(refit_score))\n",
    "    print (pd.DataFrame(confusion_matrix(y_test, y_pred), columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n",
    "    \n",
    "    # see all the scores for the param grid. Then you can also choose the combination form the grid which max ur score\n",
    "    results = pd.DataFrame (grid_search.cv_results_)\n",
    "    results = results.sort_values (by='mean_test_precision_score', ascending=False)\n",
    "    print (results)\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
