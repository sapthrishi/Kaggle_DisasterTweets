{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, KernelPCA, LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import random  \n",
    "\n",
    "from random import sample \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vars:\n",
    "RANDOM_STATE     = 0\n",
    "KBEST_FEATURES   = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTransformer (BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit (self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        if type (X) == scipy.sparse.csr.csr_matrix:\n",
    "            return X.todense ()\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit (self, X, y=None):\n",
    "        print ('IdentityTransformer: type(X), X.shape =', type (X), X.shape)\n",
    "        return self\n",
    "    \n",
    "    def transform (self, X, y=None):\n",
    "        print ('IdentityTransformer: type(X), X.shape =', type (X), X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find optimal PCA dims and the same no. of NMF features\n",
    "The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction.\n",
    "We use a GridSearchCV to set the dimensionality of the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA_NMF_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=[0.90, 0.95, 0.99], C=[0.1, 1, 10], isNMF=True):\n",
    "        \n",
    "        self.pca = PCA (whiten=True)\n",
    "        self.nmf = None\n",
    "        self.n_components = n_components\n",
    "        self.C   = C\n",
    "        self.isNMF = isNMF\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        print ('Find optimal PCA dims and the same no. of NMF features for X.shape =', X.shape)\n",
    "        # logistic = SGDClassifier(loss='log', penalty='l2', max_iter=10000, tol=1e-5, random_state=0)\n",
    "        logistic = LogisticRegression (penalty=\"l2\", class_weight='balanced')\n",
    "        pipeline = Pipeline (steps=[('to_dense', DenseTransformer()), ('pca', self.pca), ('logistic', logistic)])\n",
    "        \n",
    "        # Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "        param_grid = {\n",
    "            'pca__n_components' : self.n_components,\n",
    "            # 'logistic__alpha' : np.logspace(-4, 4, 5),  # for logistic - SGDClassifier\n",
    "            'logistic__C'       : self.C,      # for LogisticRegression\n",
    "            'logistic__fit_intercept' : [True,False]      # for LogisticRegression\n",
    "        }\n",
    "        \n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5)\n",
    "        gridSearchCV.fit (X, Y)  # Does not automatically sets the params of the pca or pipe\n",
    "        print (\"Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        \n",
    "        # Now use the optimal params to fit the PCA\n",
    "        self.pca = PCA (n_components=gridSearchCV.best_params_['pca__n_components'], whiten=True)\n",
    "        if type (X) == scipy.sparse.csr.csr_matrix:\n",
    "            X = X.todense()\n",
    "        self.pca.fit (X)\n",
    "        pcaDim = self.pca.transform (X[:2,:]).shape[1]\n",
    "        # or simply use to get already fitted best estimator: self.pca = gridSearchCV.best_estimator_\n",
    "        print(\"PCA dimensionality, explainedVarRatio = \", pcaDim, self.pca.n_components)\n",
    "        # global RESULTS\n",
    "        # RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        \n",
    "        if self.isNMF :\n",
    "            self.nmf = NMF (n_components=pcaDim, random_state=1, alpha=.1, l1_ratio=.5)\n",
    "            self.nmf.fit (X)        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        if type (X) == scipy.sparse.csr.csr_matrix:\n",
    "            X = X.todense()\n",
    "        X_pca = self.pca.transform (X)\n",
    "        X_nmf = self.nmf.transform (X)\n",
    "        if X.ndim==1 :\n",
    "            return np.concatenate ([X_pca, X_nmf])\n",
    "        return np.hstack ([X_pca, X_nmf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD_NMF_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=[0.001, 0.01, 0.1], C=[0.1, 1, 10], isNMF=True):\n",
    "        \n",
    "        self.svd = TruncatedSVD ()\n",
    "        self.nmf = None\n",
    "        self.n_components = n_components\n",
    "        self.C   = C\n",
    "        self.isNMF = isNMF\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        print ('Find optimal SVD dims and the same no. of NMF features for X.shape =', X.shape)\n",
    "        self.n_components = [int (i * X.shape[1]) for i in self.n_components if int (i * X.shape[1]) > 0]\n",
    "        # if too many components then limit upto 3000 due to memory constraints\n",
    "        if self.n_components[-1] > 3000:\n",
    "            self.n_components = [100, 800, 2000, 3000]\n",
    "        # logistic = SGDClassifier(loss='log', penalty='l2', max_iter=10000, tol=1e-5, random_state=0)\n",
    "        logistic = LogisticRegression (penalty=\"l2\", class_weight='balanced')\n",
    "        pipeline = Pipeline (steps=[('svd', self.svd), ('logistic', logistic)])\n",
    "        \n",
    "        # Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "        param_grid = {\n",
    "            'svd__n_components' : self.n_components,\n",
    "            'logistic__C'       : self.C,      # for LogisticRegression\n",
    "            'logistic__fit_intercept' : [True,False]      # for LogisticRegression\n",
    "        }\n",
    "        \n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5)\n",
    "        gridSearchCV.fit (X, Y)  # Does not automatically sets the params of the pca or pipe\n",
    "        print (\"Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        \n",
    "        # Now use the optimal params to fit the SVD\n",
    "        self.svd = TruncatedSVD (n_components=gridSearchCV.best_params_['svd__n_components'])\n",
    "        self.svd.fit (X)\n",
    "        svdDim = self.svd.transform (X[:2,:]).shape[1]\n",
    "        # or simply use to get already fitted best estimator: self.pca = gridSearchCV.best_estimator_\n",
    "        print(\"SVD dimensionality, explainedVarRatio = \", svdDim, self.svd.explained_variance_ratio_.sum())\n",
    "        # global RESULTS\n",
    "        # RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        \n",
    "        if self.isNMF :\n",
    "            self.nmf = NMF (n_components=svdDim, random_state=1, alpha=.1, l1_ratio=.5)\n",
    "            self.nmf.fit (X)        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        X_svd = self.svd.transform (X)\n",
    "        X_nmf = self.nmf.transform (X)\n",
    "        if X.ndim==1 :\n",
    "            return np.concatenate ([X_svd, X_nmf])\n",
    "        return np.hstack ([X_svd, X_nmf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICA_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=[0.30, 0.40, 0.50, 0.60], C=[0.1, 1, 10]):\n",
    "        \n",
    "        self.ica = FastICA (random_state=0, whiten=True)\n",
    "        self.n_components = n_components\n",
    "        self.C = C\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        print ('ICA_FeatureTransformer: type(X), X.shape =', type(X), X.shape)\n",
    "        logistic = LogisticRegression (penalty=\"l2\", class_weight='balanced')\n",
    "        pipeline = Pipeline(steps=[('to_dense', DenseTransformer()), ('ica', self.ica), ('logistic', logistic)])\n",
    "        self.n_components = [int (i* X.shape[1]) for i in self.n_components if int (i* X.shape[1]) > 0]\n",
    "        self.n_components = [nc for nc in self.n_components if nc>0]\n",
    "        if not self.n_components:\n",
    "            self.n_components = [2]\n",
    "        \n",
    "        # Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "        param_grid = {\n",
    "            'ica__n_components' : self.n_components,\n",
    "            # 'logistic__alpha' : np.logspace (-4, 4, 5),  # for logistic - SGDClassifier\n",
    "            'logistic__C'       : self.C,      # for LogisticRegression\n",
    "            'logistic__fit_intercept' : [True, False]      # for LogisticRegression\n",
    "        }\n",
    "        \n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5)\n",
    "        gridSearchCV.fit (X, Y)  # Does not automatically sets the params of the pca or pipe\n",
    "        print (\"Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        \n",
    "        # Now use the optimal params to fit the PCA\n",
    "        self.ica = FastICA (n_components=gridSearchCV.best_params_['ica__n_components'], whiten=True)\n",
    "        if type (X) == scipy.sparse.csr.csr_matrix:\n",
    "            X = X.todense ()\n",
    "        self.ica.fit (X)\n",
    "        icaDim = self.ica.transform (X[:2,:]).shape[1]\n",
    "        # or simply use to get already fitted best estimator: self.pca = gridSearchCV.best_estimator_\n",
    "        print (\"ICA dimensionality, explainedVarRaio = \", icaDim, self.ica.n_components)\n",
    "        # global RESULTS\n",
    "        # RESULTS.append (benchmark (gridSearchCV.best_estimator_))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        if type (X) == scipy.sparse.csr.csr_matrix:\n",
    "            X = X.todense ()\n",
    "        X_ica = self.ica.transform (X)\n",
    "        return X_ica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICASparse_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=[0.30, 0.40, 0.50, 0.60], C=[0.1, 1, 10]):\n",
    "        \n",
    "        self.ica = FastICA (random_state=0, whiten=True)\n",
    "        self.n_components = n_components\n",
    "        self.C = C\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        print ('ICA_FeatureTransformer: type(X), X.shape =', type (X), X.shape)\n",
    "        logistic = LogisticRegression (penalty=\"l2\", class_weight='balanced')\n",
    "        pipeline = Pipeline(steps=[('ica', self.ica), ('logistic', logistic)])\n",
    "        self.n_components = [int (i*X.shape[1]) for i in self.n_components if int (i*X.shape[1]) > 0]\n",
    "        if not self.n_components:\n",
    "            self.n_components = [1]\n",
    "        \n",
    "        # Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "        param_grid = {\n",
    "            'ica__n_components' : self.n_components,\n",
    "            # 'logistic__alpha' : np.logspace (-4, 4, 5),  # for logistic - SGDClassifier\n",
    "            'logistic__C'       : self.C,      # for LogisticRegression\n",
    "            'logistic__fit_intercept' : [True, False]      # for LogisticRegression\n",
    "        }\n",
    "        \n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5)\n",
    "        gridSearchCV.fit (X, Y)  # Does not automatically sets the params of the pca or pipe\n",
    "        print (\"Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        \n",
    "        # Now use the optimal params to fit the PCA\n",
    "        self.ica = FastICA (n_components=gridSearchCV.best_params_['ica__n_components'], whiten=True)\n",
    "        self.ica.fit (X)\n",
    "        icaDim = self.ica.transform (X[:2,:]).shape[1]\n",
    "        # or simply use to get already fitted best estimator: self.pca = gridSearchCV.best_estimator_\n",
    "        print (\"ICA dimensionality, explainedVarRaio = \", icaDim, self.ica.n_components)\n",
    "        # global RESULTS\n",
    "        # RESULTS.append (benchmark (gridSearchCV.best_estimator_))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        X_ica = self.ica.transform (X)\n",
    "        return X_ica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find K-best features based on Mutual-Info / F-Score / Chi^2\n",
    "Use K-best feature selection with logistic regression classifier in GridSearchCV to find optimal val of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1c9b7d23268d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mSelectKBest_FeatureTransformer\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.70\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.90\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.95\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchi2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmutual_info_classif\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_func\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mscore_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BaseEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "class SelectKBest_feature_selector (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=[0.6, 0.70, 0.80, 0.90, 0.95, 1.0], score_func=[chi2, mutual_info_classif]):\n",
    "        \n",
    "        self.score_func   = score_func\n",
    "        self.n_components = n_components\n",
    "        self.selectKBest  = SelectKBest()\n",
    "        self.shift_k      = 0\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        print ('Find K-best features based on Mutual-Info / Chi^2 for X.shape =', X.shape)\n",
    "        # this works only when X >= 0, hence shift by a constant so that it is >=0\n",
    "        self.shift_k = np.abs (X.min()) * 1.5 + 1\n",
    "        X = X + self.shift_k\n",
    "        # convert fractions to int feature count\n",
    "        self.n_components = ((np.array(self.n_components) * X.shape[1]).astype (int)).tolist ()\n",
    "        self.n_components = [nc for nc in self.n_components if nc>0]\n",
    "        if not self.n_components:\n",
    "            self.n_components = [1]\n",
    "        # logistic = SGDClassifier(loss='log', penalty='l2', max_iter=10000, tol=1e-5, random_state=0)\n",
    "        logistic = LogisticRegression (penalty=\"l2\", class_weight='balanced', C=0.1)\n",
    "        pipeline = Pipeline (steps=[('selectKBest', self.selectKBest), ('logistic', logistic)])\n",
    "        \n",
    "        # Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "        param_grid = {            \n",
    "            'selectKBest__k'          : self.n_components,\n",
    "            'selectKBest__score_func' : self.score_func\n",
    "        }\n",
    "        \n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5)\n",
    "        gridSearchCV.fit (X, Y)  # Does not automatically sets the params of the models oe pipeline\n",
    "        print (\"Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        \n",
    "        # Now use the optimal params to fit the model\n",
    "        self.k          = gridSearchCV.best_params_['selectKBest__k']\n",
    "        self.score_func = gridSearchCV.best_params_['selectKBest__score_func']\n",
    "        print (\"SelectKBest: k, score_func =\", self.k, self.score_func)\n",
    "        # print(gridSearchCV.grid_scores_)\n",
    "        \n",
    "        # Now create the model with the best params and fit over the data\n",
    "        self.selectKBest = SelectKBest (self.score_func, self.k)\n",
    "        self.selectKBest.fit (X, Y)\n",
    "        # or simply use to get already fitted best estimator: self.selectKBest = gridSearchCV.best_estimator_\n",
    "        # global RESULTS\n",
    "        # RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        return self\n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        return self.selectKBest.transform (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Through L1-regularized SVM similar to Lasso, identify the useful sparse features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseSVM_feature_selector (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, C=[10.0, 100.0]): \n",
    "        \n",
    "        self.C   = C\n",
    "        self.sfm = None\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        print ('LinearSVC with L1-based feature selection for X.shape =', X.shape)\n",
    "        # The smaller C, the stronger the regularization.\n",
    "        # The more regularization, the more sparsity.\n",
    "        pipeline = Pipeline ([\n",
    "          ('feature_selection', SelectFromModel (LinearSVC (penalty=\"l1\", dual=False, tol=1e-3))),\n",
    "          ('classification', LinearSVC(penalty=\"l2\"))  ])\n",
    "        \n",
    "        # Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "        param_grid = {            \n",
    "            'feature_selection__estimator__C' : self.C\n",
    "        }\n",
    "        \n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5)\n",
    "        print ('X.shape =', X.shape,  'Y.shape =', Y.shape)\n",
    "        gridSearchCV.fit (X, Y)  # Does not automatically sets the params of the models\n",
    "        print (\"Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        \n",
    "        # Now use the optimal params to fit the model\n",
    "        self.C   = gridSearchCV.best_params_['feature_selection__estimator__C']\n",
    "        print (\"Best feature_selection__estimator__C =\", self.C)\n",
    "        svc = LinearSVC (penalty=\"l1\", dual=False, tol=1e-3, C=self.C)\n",
    "        self.sfm = SelectFromModel (svc) #, threshold=0.0001)\n",
    "        self.sfm.fit (X, Y)\n",
    "        # or simply use to get already fitted best estimator: self.sfm = gridSearchCV.best_estimator_\n",
    "        print ('New #features = ', self.sfm.transform (X[:2,:]).shape[1] )\n",
    "        # global RESULTS\n",
    "        # RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        return self.sfm.transform (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA features\n",
    "The KPCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction. We use a GridSearchCV to set the dimensionality of the KPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPCA_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=[0.70, 0.75, 0.77], kernel=['poly', 'rbf', 'cosine']):\n",
    "        \"\"\"\n",
    "        A high value for n_components such as 0.80 leads to some errors sometimes. Hence don't exceed 0.77\n",
    "        \"\"\"\n",
    "        \n",
    "        self.kpca         = KernelPCA(remove_zero_eig=True)\n",
    "        self.n_components = n_components\n",
    "        self.kernel       = kernel \n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        print ('Kernel PCA features for X.shape =', X.shape)\n",
    "        # convert fractions to int feature count\n",
    "        self.n_components = ((np.array (self.n_components) * KernelPCA (kernel='sigmoid').fit_transform (X).shape[1]).astype (int)).tolist ()\n",
    "        self.n_components = [nc for nc in self.n_components if nc>0]\n",
    "        if not self.n_components:\n",
    "            self.n_components = [1]\n",
    "        # logistic = SGDClassifier(loss='log', penalty='l2', max_iter=10000, tol=1e-5, random_state=0)\n",
    "        logistic = LogisticRegression (penalty=\"l2\", class_weight='balanced', C=0.1)\n",
    "        pipeline = Pipeline (steps=[('to_dense', DenseTransformer()), ('kpca', self.kpca), ('logistic', logistic)])\n",
    "        \n",
    "        # Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "        param_grid = {\n",
    "            'kpca__n_components' : self.n_components,            \n",
    "            'kpca__kernel'       : self.kernel\n",
    "        }\n",
    "        \n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5)\n",
    "        gridSearchCV.fit (X, Y)  # Does not automatically sets the params of the pca\n",
    "        print (\"Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        \n",
    "        # Now use the optimal params to fit the KPCA\n",
    "        self.kpca = KernelPCA (n_components=gridSearchCV.best_params_['kpca__n_components'], \n",
    "                               kernel=gridSearchCV.best_params_['kpca__kernel'],\n",
    "                               remove_zero_eig=True)\n",
    "        if type (X) == scipy.sparse.csr.csr_matrix:\n",
    "            X = X.todense ()\n",
    "        self.kpca.fit (X)           \n",
    "        # or simply use to get already fitted best estimator: self.kpca = gridSearchCV.best_estimator_\n",
    "        # global RESULTS\n",
    "        # RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \"\"\"\n",
    "        X = sparse matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        if type (X) == scipy.sparse.csr.csr_matrix:\n",
    "            X = X.todense ()\n",
    "        return self.kpca.transform (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPCASparse_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=[0.70, 0.75, 0.77], kernel=['poly', 'rbf', 'cosine']):\n",
    "        \"\"\"\n",
    "        A high value for n_components such as 0.80 leads to some errors sometimes. Hence don't exceed 0.77\n",
    "        \"\"\"\n",
    "        \n",
    "        self.kpca         = KernelPCA(remove_zero_eig=True)\n",
    "        self.n_components = n_components\n",
    "        self.kernel       = kernel \n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        print ('Kernel PCA features for X.shape =', X.shape)\n",
    "        # convert fractions to int feature count\n",
    "        self.n_components = ((np.array (self.n_components) * KernelPCA (kernel='sigmoid').fit_transform (X).shape[1]).astype (int)).tolist ()\n",
    "        self.n_components = [nc for nc in self.n_components if nc>0]\n",
    "        if not self.n_components:\n",
    "            self.n_components = [1]\n",
    "        # logistic = SGDClassifier(loss='log', penalty='l2', max_iter=10000, tol=1e-5, random_state=0)\n",
    "        logistic = LogisticRegression (penalty=\"l2\", class_weight='balanced', C=0.1)\n",
    "        pipeline = Pipeline (steps=[('kpca', self.kpca), ('logistic', logistic)])\n",
    "        \n",
    "        # Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "        param_grid = {\n",
    "            'kpca__n_components' : self.n_components,            \n",
    "            'kpca__kernel'       : self.kernel\n",
    "        }\n",
    "        \n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5)\n",
    "        gridSearchCV.fit (X, Y)  # Does not automatically sets the params of the pca\n",
    "        print (\"Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        \n",
    "        # Now use the optimal params to fit the KPCA\n",
    "        self.kpca = KernelPCA (n_components=gridSearchCV.best_params_['kpca__n_components'], \n",
    "                               kernel=gridSearchCV.best_params_['kpca__kernel'],\n",
    "                               remove_zero_eig=True)\n",
    "        self.kpca.fit (X)           \n",
    "        # or simply use to get already fitted best estimator: self.kpca = gridSearchCV.best_estimator_\n",
    "        # global RESULTS\n",
    "        # RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \"\"\"\n",
    "        X = sparse matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.kpca.transform (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation Features\n",
    "The LDA does an unsupervised dimensionality reduction, while the logistic regression does the prediction. We use a GridSearchCV to set the dimensionality of the LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA_FeatureTransformer (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components=[0.01, 0.1, 0.5, 1.0]):\n",
    "        \n",
    "        self.lda          = LatentDirichletAllocation(max_iter=5,\n",
    "                                learning_method='online', learning_offset=50., random_state=0)\n",
    "        self.n_components = n_components\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):\n",
    "        \n",
    "        print ('LDA Features for X.shape =', X.shape)\n",
    "        # convert fractions to int feature count\n",
    "        self.n_components = ((np.array(self.n_components) * min (X.shape[0],X.shape[1])).astype (int)).tolist ()\n",
    "        self.n_components = [nc for nc in self.n_components if nc>0]\n",
    "        if not self.n_components:\n",
    "            self.n_components = [1]\n",
    "        \n",
    "        # logistic = SGDClassifier(loss='log', penalty='l2', max_iter=10000, tol=1e-5, random_state=0)\n",
    "        logistic = LogisticRegression (penalty=\"l2\", class_weight='balanced', C=0.1)\n",
    "        pipeline = Pipeline (steps=[('LDA', self.lda), ('logistic', logistic)])\n",
    "        \n",
    "        # Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "        param_grid = {\n",
    "            'LDA__n_components' : self.n_components\n",
    "        }\n",
    "        \n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5)\n",
    "        gridSearchCV.fit (X, Y)  # Does not automatically sets the params of the pca\n",
    "        print (\"Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        \n",
    "        # Now use the optimal params to fit the LDA\n",
    "        n_components = gridSearchCV.best_params_['LDA__n_components']\n",
    "        self.lda     = LatentDirichletAllocation (n_components=n_components, max_iter=5,\n",
    "                                learning_method='online', learning_offset=50., random_state=0)\n",
    "        self.lda.fit (X) \n",
    "        # or simply use to get already fitted best estimator: self.lda = gridSearchCV.best_estimator_\n",
    "        # global RESULTS\n",
    "        # RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        return self.lda.transform (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection based on clf.feature_importances_ with CV on threshold\n",
    "clf = RandomForest / Xgboost / CatBoost\n",
    "we use CV to determine threshold for SelectFromModel( rf_clf, threshold=? )\n",
    "\n",
    "@param: clf: an already fitted(X,Y) clf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLF_importance_feature_selector (BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, threshold=[0.0005, 0.001, 0.005]):\n",
    "        \n",
    "        self.clf             = RandomForestClassifier ()\n",
    "        self.selectFromModel = None\n",
    "        self.threshold       = threshold\n",
    "        return\n",
    "\n",
    "    def fit (self, X, Y, **fit_params):       \n",
    "        \n",
    "        print ('Feature Selection based on rf.feature_importances_ for X.shape =', X.shape)\n",
    "        pipeline = Pipeline ([\n",
    "          ('feature_selection', SelectFromModel (self.clf)),\n",
    "          ('classification', LinearSVC(penalty=\"l2\"))  ])\n",
    "        \n",
    "        # Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "        param_grid = {\n",
    "            'feature_selection__threshold' : self.threshold\n",
    "        }\n",
    "        \n",
    "        gridSearchCV = GridSearchCV (pipeline, param_grid, iid=False, cv=5)\n",
    "        gridSearchCV.fit (X, Y)  \n",
    "        print (\"Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)        \n",
    "        # self.clf = gridSearchCV.best_estimator_\n",
    "        \n",
    "        # self.clf.fit(X,Y) # since we aleady get a fitted clf\n",
    "        self.selectFromModel = SelectFromModel (self.clf, gridSearchCV.best_params_['feature_selection__threshold'])\n",
    "        self.selectFromModel.fit (X, Y)\n",
    "        clfDim = self.selectFromModel.transform (X[:2,:]).shape[1]        \n",
    "        print ('clfDim = ', clfDim)                \n",
    "        # global RESULTS\n",
    "        # RESULTS.append (benchmark (gridSearchCV.best_estimator_))\n",
    "        return self\n",
    "        \n",
    "    def transform (self, X, y=None, **fit_params):\n",
    "        \n",
    "        return self.selectFromModel.transform (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada-Boosted Classifiers from a base clf after CV over boosting params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ab_params     = { 'n_estimators'  :  50 }\n",
    "# ab_param_grid = { 'learning_rate' : [0.5, 0.75, 1.0] }\n",
    "\n",
    "def get_clf_adaBoosted_cv (X, Y, clf, params=None, param_grid=None):\n",
    "    \n",
    "    ab_clf = AdaBoostClassifier(base_estimator=clf)\n",
    "    if params:\n",
    "        ab_clf.set_params(**params)\n",
    "    \n",
    "    if param_grid:\n",
    "        gridSearchCV = GridSearchCV (ab_clf, param_grid, iid=False, cv=5)\n",
    "        gridSearchCV.fit (X, Y)  \n",
    "        print (\"get_clf_adaBoosted_cv(): Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        ab_clf = gridSearchCV.best_estimator_\n",
    "        global RESULTS\n",
    "        RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        # TODO: Plot scores for each split, and get its' variance\n",
    "    \n",
    "    return ab_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get optimal Classifier after CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Best_clf_cv_transformer (BaseEstimator, TransformerMixin): \n",
    "            \n",
    "    def __init__(self, params):\n",
    "        \n",
    "        self.cv    = 5\n",
    "        if 'cv' in params:\n",
    "            self.cv= int(params['cv'])\n",
    "        clf        =  None\n",
    "        name       =  params['name']\n",
    "        if name   == 'Logit':\n",
    "            clf    =  LogisticRegression ()\n",
    "        elif name == 'DT':\n",
    "            clf    =  DecisionTreeClassifier ()\n",
    "        elif name == 'RidgClf':\n",
    "            clf    =  RidgeClassifier ()\n",
    "        elif name == 'Prcpt':\n",
    "            clf    =  Perceptron ()\n",
    "        elif name == 'PssAggClf':\n",
    "            clf    =  PassiveAggressiveClassifier ()\n",
    "        elif name == 'Knn':\n",
    "            clf    =  KNeighborsClassifier ()\n",
    "        elif name == 'RF':\n",
    "            clf    =  RandomForestClassifier ()\n",
    "        elif name == 'NearCent':\n",
    "            clf    =  NearestCentroid ()\n",
    "        elif name == 'MultNB':\n",
    "            clf    =  MultinomialNB ()\n",
    "        elif name == 'BernNB':\n",
    "            clf    =  BernoulliNB ()    \n",
    "        elif name == 'Svc':\n",
    "            clf    =  SVC (probability=True)\n",
    "        elif name == 'LSvc':\n",
    "            clf    =  LinearSVC ()\n",
    "        elif name == 'Xgb':\n",
    "            clf    =  xgb.XGBClassifier() # XGBRFClassifier()\n",
    "        elif name == 'Catb' :  # issues with CV\n",
    "            clf    =  CatBoostClassifier()\n",
    "        else:\n",
    "            print('ERROR Get_best_clf_cv(): invalid @param name \\n')\n",
    "        \n",
    "        if 'params' in params:\n",
    "            clf.set_params(**params['params'])\n",
    "        self.param_grid = None\n",
    "        if 'param_grid' in params:\n",
    "            self.param_grid = params['param_grid']\n",
    "        self.clf = clf\n",
    "        self.cv_score = 0\n",
    "        self.name = name\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def fit (self, X, Y):\n",
    "        \n",
    "        print ('training', self.name, 'for X.shape =', X.shape)\n",
    "        if self.param_grid:\n",
    "            \n",
    "            gridSearchCV = GridSearchCV (self.clf, self.param_grid, iid=False, cv=self.cv)\n",
    "            gridSearchCV.fit (X, Y)  \n",
    "            print (self.name, \": get_best_clf_cv(): Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "            print (gridSearchCV.best_params_)\n",
    "            self.clf = gridSearchCV.best_estimator_\n",
    "            self.cv_score = gridSearchCV.best_score_\n",
    "            # global RESULTS\n",
    "            # RESULTS.append(benchmark(self.clf))\n",
    "            # TODO: Plot scores for each split, and get its' variance\n",
    "        else:\n",
    "            \n",
    "            self.clf.fit (X,Y) \n",
    "            predY = self.clf.predict (X)\n",
    "            self.cv_score = metrics.accuracy_score (Y, predY)\n",
    "            print(self.name, \": accuracy:   %0.3f\" % self.cv_score)\n",
    "        return self\n",
    "    \n",
    "    def get_cv_score (self):\n",
    "        return self.cv_score\n",
    "    \n",
    "    def transform (self, X, Y=None, **fit_params):\n",
    "        return self.clf.transform(X, Y)\n",
    "    \n",
    "    def predict (self, X, **fit_params):\n",
    "        return self.clf.predict(X)\n",
    "    \n",
    "    def predict_proba (self, X):\n",
    "        return self.clf.predict_proba (X)\n",
    "    \n",
    "    def predict_log_proba (self, X):\n",
    "        return self.clf.predict_log_proba (X)\n",
    "    \n",
    "    def score (self, X, Y, **fit_params):\n",
    "        return self.clf.score(X, Y)\n",
    "    \n",
    "    def decision_function (self, X, **fit_params):\n",
    "        return self.clf.decision_function (X)\n",
    "    \n",
    "    def set_params (self, **params):\n",
    "        return self.clf.set_params(**params)\n",
    "    \n",
    "    def get_params(self, deep=None):\n",
    "        return self.clf.get_params(deep)\n",
    "    \n",
    "    def apply (self, X):\n",
    "        return self.clf.apply(X)\n",
    "    \n",
    "    def decision_path (self, X):\n",
    "        return self.clf.decision_path (X)\n",
    "    \n",
    "    def staged_decision_function (self, X):\n",
    "        return self.clf.staged_decision_function (X)\n",
    "    \n",
    "    def staged_predict (self, X):\n",
    "        return self.clf.staged_predict (X)\n",
    "    \n",
    "    def staged_predict_proba (self, X):\n",
    "        return self.clf.staged_predict_proba (X)\n",
    "    \n",
    "    def staged_score (self, X):\n",
    "        return self.clf.staged_score (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bagging Classifier from a base clf, after CV on boosting params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params     = { 'max_samples'  : 1.0,  'n_estimators' : 10 }\n",
    "# param_grid = { 'max_features' : [0.7, 0.8, 0.9, 1.0] }\n",
    "\n",
    "def get_bagging_clf_cv (X, Y, clf, params=None, param_grid=None): \n",
    "    \n",
    "    bag_clf = BaggingClassifier(base_estimator=clf)\n",
    "    if params:\n",
    "        bag_clf.set_params(**params)\n",
    "    \n",
    "    if param_grid:\n",
    "        gridSearchCV = GridSearchCV (bag_clf, param_grid, iid=False, cv=5)\n",
    "        gridSearchCV.fit (X, Y)  \n",
    "        print (\"get_bagging_clf_cv(): Best parameter (CV score=%0.3f):\" % gridSearchCV.best_score_)\n",
    "        print (gridSearchCV.best_params_)\n",
    "        bag_clf = gridSearchCV.best_estimator_\n",
    "        global RESULTS\n",
    "        RESULTS.append(benchmark(gridSearchCV.best_estimator_))\n",
    "        # TODO: Plot scores for each split, and get its' variance\n",
    "    \n",
    "    return bag_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def getBaggedXGB_RESULTS(Xtrain, Xtest, Ytrain):\n",
    "    \n",
    "    param = {}\n",
    "    param['booster'] = 'gbtree'\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['num_class'] = 9\n",
    "    param['eval_metric'] = 'logloss'\n",
    "    param['scale_pos_weight'] = 1.0\n",
    "    param['bst:eta'] = 0.3\n",
    "    param['bst:max_depth'] = 6\n",
    "    param['bst:colsample_bytree'] = 0.5\n",
    "    param['silent'] = 1\n",
    "    param['nthread'] = 16\n",
    "    num_round = 100\n",
    "    plst = list(param.items())\n",
    "    watchlist = []\n",
    "    \n",
    "    time0 = time()\n",
    "    idxTrain = range(len(Ytrain))\n",
    "    Ytestxg  = np.zeros((Xtest.shape[0], 9))\n",
    "    \n",
    "    bgs = 2 # 20\n",
    "    for bg in range(bgs):\n",
    "        param['seed'] = bg + 1\n",
    "        plst = list(param.items())\n",
    "        \n",
    "        newidxTrain = random.sample(idxTrain, int(len(idxTrain) * 1.0))\n",
    "        \n",
    "        for i in range(int(len(idxTrain) * 7.0)):\n",
    "            newidxTrain.append(random.choice(idxTrain))\n",
    "        \n",
    "        Xdatatrain = xgb.DMatrix(data = Xtrain[newidxTrain], label = Ytrain[newidxTrain])\n",
    "        Xdatatest = xgb.DMatrix(data = Xtest)\n",
    "        \n",
    "        bst = xgb.train(plst, Xdatatrain, num_round, watchlist)\n",
    "        \n",
    "        curpred = bst.predict(Xdatatest).reshape((Xtest.shape[0], 9))        \n",
    "        Ytestxg += curpred\n",
    "        \n",
    "        print (bg, (time() - time0) / 60.)\n",
    "        \n",
    "    Ytestxg /= bgs\n",
    "    return Ytestxg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ytestxg = getBaggedXGB_RESULTS(X_TRAIN, X_TEST, Y_TRAIN)\n",
    "Ytestxg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 5]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[int (i*10) for i in [0.1, 0.2, 0.5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
